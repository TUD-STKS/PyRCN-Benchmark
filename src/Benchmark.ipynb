{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "033f4152-7dff-4ce3-a0da-26dee5aa3e45",
   "metadata": {},
   "source": [
    "# Benchmark PyRCN\n",
    "\n",
    "This notebook accompanies the publication \"PyRCN: A Toolbox for Exploration and Application of Reservoir Computing Networks\" and serves as a benchmark test for PyRCN and other libraries.\n",
    "\n",
    "Technical analysis -> Chart Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e078dadd-e54c-42e2-a2a4-786534e52b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import MinMaxScaler, FunctionTransformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.utils.fixes import loguniform\n",
    "from scipy.stats import uniform\n",
    "from joblib import dump, load\n",
    "\n",
    "from pyrcn.extreme_learning_machine import ELMRegressor\n",
    "from pyrcn.echo_state_network import ESNRegressor\n",
    "from pyrcn.model_selection import SequentialSearchCV\n",
    "from skelm import ELMRegressor as SkELMRegressor\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from preprocessing import ts2super, compute_average_volatility\n",
    "from model_selection import PredefinedTrainValidationTestSplit\n",
    "from adapter import PyESN\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('png', 'pdf')\n",
    "sns.set_theme(context=\"paper\", style=\"whitegrid\", font=\"serif\", font_scale=.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab7fa07-7dab-412d-b044-de201098c590",
   "metadata": {},
   "source": [
    "## Prepare all datasets\n",
    "\n",
    "We load the datasets as pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d682b5dc-6632-4184-8dac-0f0fe2f42ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = OrderedDict({\"CAT\": 0, \"EBAY\": 1, \"MSFT\": 2})\n",
    "data = [None] * len(datasets)\n",
    "\n",
    "for dataset, k in datasets.items():\n",
    "    data[k] = pd.read_csv(f\"../data/{dataset}.csv\")[\"t0\"].to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532cb2f1-f95d-41be-8ee8-54df39675d3b",
   "metadata": {},
   "source": [
    "Next, we create feature extraction pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e741b8-5778-4b3f-b7bb-10b599e4512d",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_volatility_transformer = FunctionTransformer(\n",
    "    func=compute_average_volatility, kw_args={\"window_length\": 1})\n",
    "week_volatility_transformer = FunctionTransformer(\n",
    "    func=compute_average_volatility, kw_args={\"window_length\": 5})\n",
    "month_volatility_transformer = FunctionTransformer(\n",
    "    func=compute_average_volatility, kw_args={\"window_length\": 22})\n",
    "har_features = FeatureUnion(\n",
    "    transformer_list=[(\"day\", day_volatility_transformer),\n",
    "                      (\"week\", week_volatility_transformer),\n",
    "                      (\"month\", month_volatility_transformer)])\n",
    "har_pipeline = Pipeline(\n",
    "    steps=[(\"har_features\", har_features),\n",
    "           (\"scaler\", MinMaxScaler()),\n",
    "           (\"lstsq\", TransformedTargetRegressor(transformer=MinMaxScaler()))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86906f83-ba62-4132-8a91-de2a933fb718",
   "metadata": {},
   "source": [
    "We define a forecasting horizon for which we want to optimize and analyze models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f1b8ce-184e-49e5-bbc9-e45c0c47b40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "H=21  # pre-trained models available for 1, 5, 21\n",
    "\n",
    "# ts2super simply returns a dataframe containing the original and shifted by H time series.\n",
    "df = pd.concat([ts2super(d, 0, H) for d in data])\n",
    "X = df.iloc[:, 0].values.reshape(-1, 1)\n",
    "y = df.iloc[:, -1].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d95bb9-633f-49c1-be23-4dd606cde60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input = df.iloc[:, 0].to_frame()\n",
    "df_input[\"t5\"] = df_input.t0.rolling(window=5, min_periods=1).mean()\n",
    "df_input[\"t22\"] = df_input.t0.rolling(window=22, min_periods=1).mean()\n",
    "df_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b53c906-7f4a-4fcc-965a-5a22a0d8af84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input_extended = pd.concat([pd.DataFrame(data={\"t0\": [0], \"t5\": [0], \"t22\": [0]}), df_input], ignore_index=False).reset_index(drop=True)\n",
    "df_input_extended[\"t0\"] = df_input_extended[\"t0\"].shift(periods=-1)\n",
    "df_input_extended = df_input_extended.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedd81c3-0037-43be-975e-3172998d8a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input_extended.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8de015-8681-406f-acaa-f367185dc536",
   "metadata": {},
   "source": [
    "Provide a custom train test split for this task, and all scores that we aim to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99e7969-b5f9-429d-910f-3c10026413e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fold = [[k] * len(ts2super(d, 0, H)) for k, d in enumerate(data)]\n",
    "test_fold = list(itertools.chain.from_iterable(test_fold))\n",
    "\n",
    "ps = PredefinedTrainValidationTestSplit(test_fold=test_fold, validation=True)\n",
    "ps_test = PredefinedTrainValidationTestSplit(test_fold=test_fold, validation=False)\n",
    "\n",
    "scoring = {\"MSE\": \"neg_mean_squared_error\", \"RMSE\": \"neg_root_mean_squared_error\", \"R2\": \"r2\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532455c7-0d67-4df7-9565-697a66b9aa06",
   "metadata": {},
   "source": [
    "## HAR\n",
    "\n",
    "The HAR (Heterogeneous AutoRegressive) features are, as the name says, autoregressive features. Here, temporal context is considered by smoothing a time series with moving averages of the last week (5 days) and month (22 days). The smoothed variants are finally added to the original time series. Thus, HAR has three features.\n",
    "\n",
    "TODO: HAR-Feature extraction outside of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdc806b-3a1f-4afe-abb5-58390d9ae9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "har_pipeline = Pipeline(\n",
    "    steps=[# (\"har_features\", har_features),\n",
    "           (\"scaler\", MinMaxScaler()),\n",
    "           (\"lstsq\", TransformedTargetRegressor(transformer=MinMaxScaler()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a8f2c3-1288-474a-bb98-4a447ee27e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search = load(f'../results/har_grid_h{H}_modified.joblib')\n",
    "except FileNotFoundError:\n",
    "    search = GridSearchCV(estimator=har_pipeline, param_grid={}, cv=ps_test,\n",
    "                          scoring=scoring, refit=\"R2\", return_train_score=True,\n",
    "                          verbose=10).fit(df_input_extended, y)\n",
    "    dump(search, f'../results/har_grid_h{H}_modified.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd8a0e4-bfb6-4801-8c3f-892eed00f9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"R^2 train\\tR^2 test\\tMSE train\\tMSE test\\tFit time in ms\\tScore time in ms\")\n",
    "print(f\"{search.cv_results_['mean_train_R2']}\\t{search.cv_results_['mean_test_R2']}\\t\"\n",
    "      f\"{search.cv_results_['mean_train_MSE']}\\t{search.cv_results_['mean_test_MSE']}\\t\"\n",
    "      f\"{search.cv_results_['mean_fit_time']*1e3}\\t{search.cv_results_['mean_score_time']*1e3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e6d329-f1fc-426e-b1c3-c2ab73e76cde",
   "metadata": {},
   "source": [
    "## PyRCN ESN\n",
    "\n",
    "Next, we optimize ESNs from PyRCN using only the original time series as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da7966a-bd89-4135-b0f2-eb39222bef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search = load(f'../results/pyrcn_seq_esn_h{H}.joblib')\n",
    "except FileNotFoundError:\n",
    "    initial_esn_params = {\n",
    "        'hidden_layer_size': 50, 'k_in': 1, 'input_scaling': 0.4,\n",
    "        'input_activation': 'identity', 'bias_scaling': 0.0,\n",
    "        'spectral_radius': 0.0, 'leakage': 1.0, 'k_rec': 10,\n",
    "        'reservoir_activation': 'tanh', 'bidirectional': False,\n",
    "        'alpha': 1e-5, 'random_state': 42}\n",
    "    esn_pipeline = Pipeline(steps=[(\"scaler\", MinMaxScaler()),\n",
    "                                   (\"esn\", TransformedTargetRegressor(\n",
    "                                       regressor=ESNRegressor(**initial_esn_params),\n",
    "                                       transformer=MinMaxScaler()))\n",
    "                                  ])\n",
    "    # Run model selection\n",
    "    step1_params = {'esn__regressor__input_scaling': uniform(loc=1e-2, scale=1),\n",
    "                    'esn__regressor__spectral_radius': uniform(loc=0, scale=2)}\n",
    "    step2_params = {'esn__regressor__leakage': uniform(1e-5, 1e0)}\n",
    "    step3_params = {'esn__regressor__bias_scaling': uniform(loc=0, scale=3)}\n",
    "    step4_params = {'esn__regressor__hidden_layer_size':\n",
    "                        [50, 100, 200, 400, 800, 1600, 3200, 6400],\n",
    "                    'esn__regressor__alpha': loguniform(1e-5, 1e1)}\n",
    "    \n",
    "    kwargs_step1 = {'n_iter': 200, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step2 = {'n_iter': 50, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step3 = {'n_iter': 50, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step4 = {'n_iter': 200, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "\n",
    "    searches = [\n",
    "        ('step1', RandomizedSearchCV, step1_params, kwargs_step1),\n",
    "        ('step2', RandomizedSearchCV, step2_params, kwargs_step2),\n",
    "        ('step3', RandomizedSearchCV, step3_params, kwargs_step3),\n",
    "        ('step4', RandomizedSearchCV, step4_params, kwargs_step4)]\n",
    "    search = SequentialSearchCV(esn_pipeline, searches=searches).fit(X, y)\n",
    "    dump(search, f'../results/pyrcn_seq_esn_h{H}.joblib')\n",
    "print(search.all_best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b4d1c7-d214-4699-ad2c-2766f0c850f7",
   "metadata": {},
   "source": [
    "We do an additional GridSearchCV without parameters, because the object returns a mass of useful information.\n",
    "\n",
    "If we do it without parameters, simply the estimator will be fit on the train test splits defined in our cv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f5e590-431a-4b17-8729-2b979b09af19",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search_test = load(f'../results/pyrcn_seq_esn_h{H}_test.joblib')\n",
    "except FileNotFoundError:\n",
    "    search_test = GridSearchCV(estimator=clone(search.best_estimator_), param_grid={}, cv=ps_test,\n",
    "                               scoring=scoring, refit=\"R2\", return_train_score=True).fit(X, y)\n",
    "    dump(search_test, f'../results/pyrcn_seq_esn_h{H}_test.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4a1be4-7a80-4d7a-a365-c8252cb2b4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"R^2 train\\tR^2 test\\tMSE train\\tMSE test\\tFit time in ms\\tScore time in ms\")\n",
    "print(f\"{search_test.cv_results_['mean_train_R2']}\\t{search_test.cv_results_['mean_test_R2']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_train_MSE']}\\t{search_test.cv_results_['mean_test_MSE']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_fit_time']*1e3}\\t{search_test.cv_results_['mean_score_time']*1e3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fcd07f-411c-4022-be8a-6081cb9ab3f6",
   "metadata": {},
   "source": [
    "Next, we optimize ESNs from PyRCN using HAR as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c42717-100d-4e5b-bae3-59334ac67a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search = load(f'../results/pyrcn_har_seq_esn_h{H}.joblib')\n",
    "except FileNotFoundError:\n",
    "    initial_esn_params = {\n",
    "        'hidden_layer_size': 50, 'k_in': 1, 'input_scaling': 0.4,\n",
    "        'input_activation': 'identity', 'bias_scaling': 0.0,\n",
    "        'spectral_radius': 0.0, 'leakage': 1.0, 'k_rec': 10,\n",
    "        'reservoir_activation': 'tanh', 'bidirectional': False,\n",
    "        'alpha': 1e-5, 'random_state': 42}\n",
    "    esn_pipeline = Pipeline(steps=[(\"har_features\", har_features),\n",
    "                                   (\"scaler\", MinMaxScaler()),\n",
    "                                   (\"esn\", TransformedTargetRegressor(\n",
    "                                       regressor=ESNRegressor(**initial_esn_params),\n",
    "                                       transformer=MinMaxScaler()))\n",
    "                                  ])\n",
    "    # Run model selection\n",
    "    step1_params = {'esn__regressor__input_scaling': uniform(loc=1e-2, scale=1),\n",
    "                    'esn__regressor__spectral_radius': uniform(loc=0, scale=2)}\n",
    "    step2_params = {'esn__regressor__leakage': uniform(1e-5, 1e0)}\n",
    "    step3_params = {'esn__regressor__bias_scaling': uniform(loc=0, scale=3)}\n",
    "    step4_params = {'esn__regressor__hidden_layer_size':\n",
    "                        [50, 100, 200, 400, 800, 1600, 3200, 6400],\n",
    "                    'esn__regressor__alpha': loguniform(1e-5, 1e1)}\n",
    "    \n",
    "    kwargs_step1 = {'n_iter': 200, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step2 = {'n_iter': 50, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step3 = {'n_iter': 50, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step4 = {'n_iter': 200, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "\n",
    "    searches = [\n",
    "        ('step1', RandomizedSearchCV, step1_params, kwargs_step1),\n",
    "        ('step2', RandomizedSearchCV, step2_params, kwargs_step2),\n",
    "        ('step3', RandomizedSearchCV, step3_params, kwargs_step3),\n",
    "        ('step4', RandomizedSearchCV, step4_params, kwargs_step4)]\n",
    "    search = SequentialSearchCV(esn_pipeline, searches=searches).fit(X, y)\n",
    "    dump(search, f'../results/pyrcn_har_seq_esn_h{H}.joblib')\n",
    "print(search.all_best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a10d2e-9b39-4dcd-9e1c-aec94209af8c",
   "metadata": {},
   "source": [
    "We do an additional GridSearchCV without parameters, because the object returns a mass of useful information.\n",
    "\n",
    "If we do it without parameters, simply the estimator will be fit on the train test splits defined in our cv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fcf3f0-ba5f-4b38-b44e-59709fc123f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search_test = load(f'../results/pyrcn_har_seq_esn_h{H}_test.joblib')\n",
    "except FileNotFoundError:\n",
    "    search_test = GridSearchCV(estimator=clone(search.best_estimator_), param_grid={}, cv=ps_test,\n",
    "                               scoring=scoring, refit=\"R2\", return_train_score=True).fit(X, y)\n",
    "    dump(search_test, f'../results/pyrcn_har_seq_esn_h{H}_test.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83ec0d2-c450-44aa-86ee-e473d0520e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"R^2 train\\tR^2 test\\tMSE train\\tMSE test\\tFit time in ms\\tScore time in ms\")\n",
    "print(f\"{search_test.cv_results_['mean_train_R2']}\\t{search_test.cv_results_['mean_test_R2']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_train_MSE']}\\t{search_test.cv_results_['mean_test_MSE']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_fit_time']*1e3}\\t{search_test.cv_results_['mean_score_time']*1e3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c42c68-5e1a-455e-94a8-d16dd375c6fa",
   "metadata": {},
   "source": [
    "## PyRCN ELM\n",
    "\n",
    "Next, we optimize ELMs from PyRCN using only the original time series as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475b9669-9977-4030-ace0-868442cf2386",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search = load(f'../results/pyrcn_seq_elm_h{H}.joblib')\n",
    "except FileNotFoundError:\n",
    "    initial_elm_params = {\n",
    "        'hidden_layer_size': 50, 'k_in': 1, 'input_scaling': 0.4,\n",
    "        'input_activation': 'tanh', 'bias_scaling': 0.0,\n",
    "        'alpha': 1e-5, 'random_state': 42}\n",
    "    elm_pipeline = Pipeline(steps=[(\"scaler\", MinMaxScaler()),\n",
    "                                   (\"elm\", TransformedTargetRegressor(\n",
    "                                       regressor=ELMRegressor(**initial_elm_params),\n",
    "                                       transformer=MinMaxScaler()))\n",
    "                                  ])\n",
    "    # Run model selection\n",
    "    step1_params = {'elm__regressor__input_scaling': uniform(loc=1e-2, scale=1)}\n",
    "    step2_params = {'elm__regressor__bias_scaling': uniform(loc=0, scale=3)}\n",
    "    step3_params = {'elm__regressor__hidden_layer_size':\n",
    "                        [50, 100, 200, 400, 800, 1600, 3200, 6400],\n",
    "                    'elm__regressor__alpha': loguniform(1e-5, 1e1)}\n",
    "    \n",
    "    kwargs_step1 = {'n_iter': 50, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step2 = {'n_iter': 50, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step3 = {'n_iter': 200, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "\n",
    "    searches = [\n",
    "        ('step1', RandomizedSearchCV, step1_params, kwargs_step1),\n",
    "        ('step2', RandomizedSearchCV, step2_params, kwargs_step2),\n",
    "        ('step3', RandomizedSearchCV, step3_params, kwargs_step3)]\n",
    "    search = SequentialSearchCV(elm_pipeline, searches=searches).fit(X, y)\n",
    "    dump(search, f'../results/pyrcn_seq_elm_h{H}.joblib')\n",
    "print(search.all_best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e0f5d6-738d-4cbe-8328-c0d1a1c04af2",
   "metadata": {},
   "source": [
    "We do an additional GridSearchCV without parameters, because the object returns a mass of useful information.\n",
    "\n",
    "If we do it without parameters, simply the estimator will be fit on the train test splits defined in our cv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51c5c19-14e5-4d42-880a-0e48b062e496",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search_test = load(f'../results/pyrcn_seq_elm_h{H}_test.joblib')\n",
    "except FileNotFoundError:\n",
    "    search_test = GridSearchCV(estimator=clone(search.best_estimator_), param_grid={}, cv=ps_test,\n",
    "                               scoring=scoring, refit=\"R2\", return_train_score=True).fit(X, y)\n",
    "    dump(search_test, f'../results/pyrcn_seq_elm_h{H}_test.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8c90d1-c9c9-4ee3-a356-2cd003e271a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"R^2 train\\tR^2 test\\tMSE train\\tMSE test\\tFit time in ms\\tScore time in ms\")\n",
    "print(f\"{search_test.cv_results_['mean_train_R2']}\\t{search_test.cv_results_['mean_test_R2']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_train_MSE']}\\t{search_test.cv_results_['mean_test_MSE']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_fit_time']*1e3}\\t{search_test.cv_results_['mean_score_time']*1e3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e59891-409d-4979-bab2-6a017fb0ff9f",
   "metadata": {},
   "source": [
    "Next, we optimize ELMs from PyRCN using HAR as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac485279-fccf-47e8-9bf7-1570101fd29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search = load(f'../results/pyrcn_har_seq_elm_h{H}.joblib')\n",
    "except FileNotFoundError:\n",
    "    initial_elm_params = {\n",
    "        'hidden_layer_size': 50, 'k_in': 1, 'input_scaling': 0.4,\n",
    "        'input_activation': 'tanh', 'bias_scaling': 0.0,\n",
    "        'alpha': 1e-5, 'random_state': 42}\n",
    "    elm_pipeline = Pipeline(steps=[(\"har_features\", har_features),\n",
    "                                   (\"scaler\", MinMaxScaler()),\n",
    "                                   (\"elm\", TransformedTargetRegressor(\n",
    "                                       regressor=ELMRegressor(**initial_elm_params),\n",
    "                                       transformer=MinMaxScaler()))\n",
    "                                  ])\n",
    "    # Run model selection\n",
    "    step1_params = {'elm__regressor__input_scaling': uniform(loc=1e-2, scale=1)}\n",
    "    step2_params = {'elm__regressor__bias_scaling': uniform(loc=0, scale=3)}\n",
    "    step3_params = {'elm__regressor__hidden_layer_size':\n",
    "                        [50, 100, 200, 400, 800, 1600, 3200, 6400],\n",
    "                    'elm__regressor__alpha': loguniform(1e-5, 1e1)}\n",
    "    \n",
    "    kwargs_step1 = {'n_iter': 50, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step2 = {'n_iter': 50, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step3 = {'n_iter': 200, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "\n",
    "    searches = [\n",
    "        ('step1', RandomizedSearchCV, step1_params, kwargs_step1),\n",
    "        ('step2', RandomizedSearchCV, step2_params, kwargs_step2),\n",
    "        ('step3', RandomizedSearchCV, step3_params, kwargs_step3)]\n",
    "    search = SequentialSearchCV(elm_pipeline, searches=searches).fit(X, y)\n",
    "    dump(search, f'../results/pyrcn_har_seq_elm_h{H}.joblib')\n",
    "print(search.all_best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa97d242-c8a2-429a-b4ea-b78a77b5cf78",
   "metadata": {},
   "source": [
    "We do an additional GridSearchCV without parameters, because the object returns a mass of useful information.\n",
    "\n",
    "If we do it without parameters, simply the estimator will be fit on the train test splits defined in our cv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d63bda-06ac-4155-b097-50ddf1a70488",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search_test = load(f'../results/pyrcn_har_seq_elm_h{H}_test.joblib')\n",
    "except FileNotFoundError:\n",
    "    search_test = GridSearchCV(estimator=clone(search.best_estimator_), param_grid={}, cv=ps_test,\n",
    "                               scoring=scoring, refit=\"R2\", return_train_score=True).fit(X, y)\n",
    "    dump(search_test, f'../results/pyrcn_har_seq_elm_h{H}_test.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c6b0a2-1660-4361-a9e5-2121108dda94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"R^2 train\\tR^2 test\\tMSE train\\tMSE test\\tFit time in ms\\tScore time in ms\")\n",
    "print(f\"{search_test.cv_results_['mean_train_R2']}\\t{search_test.cv_results_['mean_test_R2']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_train_MSE']}\\t{search_test.cv_results_['mean_test_MSE']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_fit_time']*1e3}\\t{search_test.cv_results_['mean_score_time']*1e3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160dc009-3ec4-4eba-8ca6-3cc64b4430bc",
   "metadata": {},
   "source": [
    "## HP-ELM\n",
    "\n",
    "Next, we optimize ELMs from HP-ELM using only the original time series as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eb0b5a-a034-4d69-be5e-3bc41efab22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search = load(f'../results/skelm_seq_elm_h{H}.joblib')\n",
    "except FileNotFoundError:\n",
    "    initial_elm_params = {'n_neurons': 50, 'density': 0.1, \n",
    "                          'ufunc': 'tanh', 'alpha': 1e-5,\n",
    "                          'random_state': 42}\n",
    "    elm_pipeline = Pipeline(steps=[(\"scaler\", MinMaxScaler()),\n",
    "                                   (\"elm\", TransformedTargetRegressor(\n",
    "                                       regressor=SkELMRegressor(**initial_elm_params),\n",
    "                                       transformer=MinMaxScaler()))\n",
    "                                  ])\n",
    "    # Run model selection\n",
    "    step1_params = {'elm__regressor__n_neurons':\n",
    "                        [50, 100, 200, 400, 800, 1600, 3200, 6400],\n",
    "                    'elm__regressor__alpha': loguniform(1e-5, 1e1)}\n",
    "    \n",
    "    kwargs_step1 = {'n_iter': 200, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "\n",
    "    searches = [\n",
    "        ('step1', RandomizedSearchCV, step1_params, kwargs_step1)]\n",
    "    search = SequentialSearchCV(elm_pipeline, searches=searches).fit(X, y)\n",
    "    dump(search, f'../results/skelm_seq_elm_h{H}.joblib')\n",
    "print(search.all_best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd523fb1-d360-4b72-a640-07cacc4831da",
   "metadata": {},
   "source": [
    "We do an additional GridSearchCV without parameters, because the object returns a mass of useful information.\n",
    "\n",
    "If we do it without parameters, simply the estimator will be fit on the train test splits defined in our cv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a60403-de08-4d60-9b48-971ecbbacef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search_test = load(f'../results/skelm_seq_elm_h{H}_test.joblib')\n",
    "except FileNotFoundError:\n",
    "    search_test = GridSearchCV(estimator=clone(search.best_estimator_), param_grid={}, cv=ps_test,\n",
    "                               scoring=scoring, refit=\"R2\", return_train_score=True).fit(X, y)\n",
    "    dump(search_test, f'../results/skelm_seq_elm_h{H}_test.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f62125-6690-42f6-9668-055ffe3d6a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"R^2 train\\tR^2 test\\tMSE train\\tMSE test\\tFit time in ms\\tScore time in ms\")\n",
    "print(f\"{search_test.cv_results_['mean_train_R2']}\\t{search_test.cv_results_['mean_test_R2']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_train_MSE']}\\t{search_test.cv_results_['mean_test_MSE']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_fit_time']*1e3}\\t{search_test.cv_results_['mean_score_time']*1e3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f045a5e3-ed10-4f0c-b2bc-e21edab91e4c",
   "metadata": {},
   "source": [
    "Next, we optimize ELMs from HP-ELM using HAR as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a14f20-03fd-4e90-a31f-467cbba3ac47",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search = load(f'../results/skelm_har_seq_elm_h{H}.joblib')\n",
    "except FileNotFoundError:\n",
    "    initial_elm_params = {'n_neurons': 200, 'density': 0.1,\n",
    "                          'ufunc': 'tanh', 'alpha': 1e-5,\n",
    "                          'random_state': 42}\n",
    "    elm_pipeline = Pipeline(steps=[(\"har_features\", har_features),\n",
    "                                   (\"scaler\", MinMaxScaler()),\n",
    "                                   (\"elm\", TransformedTargetRegressor(\n",
    "                                       regressor=SkELMRegressor(**initial_elm_params),\n",
    "                                       transformer=MinMaxScaler()))\n",
    "                                  ])\n",
    "    # Run model selection\n",
    "    step1_params = {'elm__regressor__n_neurons':\n",
    "                        [50, 100, 200, 400, 800, 1600, 3200, 6400],\n",
    "                    'elm__regressor__alpha': loguniform(1e-5, 1e1)}\n",
    "    \n",
    "    kwargs_step1 = {'n_iter': 200, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "\n",
    "    searches = [\n",
    "        ('step1', RandomizedSearchCV, step1_params, kwargs_step1)]\n",
    "    search = SequentialSearchCV(elm_pipeline, searches=searches).fit(X, y)\n",
    "    dump(search, f'../results/skelm_har_seq_elm_h{H}.joblib')\n",
    "print(search.all_best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e55b881-821b-476f-9a13-3740dbd9bc51",
   "metadata": {},
   "source": [
    "We do an additional GridSearchCV without parameters, because the object returns a mass of useful information.\n",
    "\n",
    "If we do it without parameters, simply the estimator will be fit on the train test splits defined in our cv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5305b0-77e0-4371-906f-7add1e3d6ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search_test = load(f'../results/skelm_har_seq_elm_h{H}_test.joblib')\n",
    "except FileNotFoundError:\n",
    "    search_test = GridSearchCV(estimator=clone(search.best_estimator_), param_grid={}, cv=ps_test,\n",
    "                               scoring=scoring, refit=\"R2\", return_train_score=True).fit(X, y)\n",
    "    dump(search_test, f'../results/skelm_har_seq_elm_h{H}_test.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858af3b1-a9c5-4598-8404-39c67036b7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"R^2 train\\tR^2 test\\tMSE train\\tMSE test\\tFit time in ms\\tScore time in ms\")\n",
    "print(f\"{search_test.cv_results_['mean_train_R2']}\\t{search_test.cv_results_['mean_test_R2']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_train_MSE']}\\t{search_test.cv_results_['mean_test_MSE']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_fit_time']*1e3}\\t{search_test.cv_results_['mean_score_time']*1e3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474a7e30-3e2f-4afe-acb9-7226fc130324",
   "metadata": {},
   "source": [
    "## PyESN ESN\n",
    "\n",
    "Next, we optimize ESNs from PyESN using only the original time series as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1b22cb-39cd-483a-bb32-9ea4f44f5347",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search = load(f'../results/pyesn_seq_esn_h{H}.joblib')\n",
    "except FileNotFoundError:\n",
    "    initial_esn_params = {'n_inputs': 1, 'n_outputs': 1, 'n_reservoir': 50,\n",
    "                          'spectral_radius': 0.0, 'sparsity': 0.5,\n",
    "                          'noise': 1e-3, 'input_shift': 0,\n",
    "                          'input_scaling': 0.4, 'teacher_forcing': True,\n",
    "                          'teacher_scaling': 1., 'teacher_shift': 0,\n",
    "                          'random_state': 42}\n",
    "    esn_pipeline = Pipeline(steps=[(\"scaler\", MinMaxScaler()),\n",
    "                                   (\"esn\", TransformedTargetRegressor(\n",
    "                                       regressor=PyESN(**initial_esn_params),\n",
    "                                       transformer=MinMaxScaler()))\n",
    "                                  ])\n",
    "    # Run model selection\n",
    "    step1_params = {'esn__regressor__input_scaling': uniform(loc=1e-2, scale=1),\n",
    "                    'esn__regressor__spectral_radius': uniform(loc=0, scale=2)}\n",
    "    step2_params = {'esn__regressor__n_reservoir':\n",
    "                        [50, 100, 200, 400, 800, 1600, 3200, 6400],\n",
    "                    'esn__regressor__noise': loguniform(1e-5, 1e1)}\n",
    "    \n",
    "    kwargs_step1 = {'n_iter': 200, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step2 = {'n_iter': 200, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "\n",
    "    searches = [\n",
    "        ('step1', RandomizedSearchCV, step1_params, kwargs_step1),\n",
    "        ('step2', RandomizedSearchCV, step2_params, kwargs_step2)]\n",
    "    search = SequentialSearchCV(esn_pipeline, searches=searches).fit(X, y)\n",
    "    dump(search, f'../results/pyesn_seq_esn_h{H}.joblib')\n",
    "print(search.all_best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d514d1-8686-46db-bfaf-9ad21c4682d4",
   "metadata": {},
   "source": [
    "We do an additional GridSearchCV without parameters, because the object returns a mass of useful information.\n",
    "\n",
    "If we do it without parameters, simply the estimator will be fit on the train test splits defined in our cv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2044c0-f435-4e1f-8658-49422d96ebca",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search_test = load(f'../results/pyesn_seq_esn_h{H}_test.joblib')\n",
    "except FileNotFoundError:\n",
    "    search_test = GridSearchCV(estimator=clone(search.best_estimator_), param_grid={}, cv=ps_test,\n",
    "                               scoring=scoring, refit=\"R2\", return_train_score=True).fit(X, y)\n",
    "    dump(search_test, f'../results/pyesn_seq_esn_h{H}_test.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378af756-3f32-4133-9121-8ec7e17a7ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"R^2 train\\tR^2 test\\tMSE train\\tMSE test\\tFit time in ms\\tScore time in ms\")\n",
    "print(f\"{search_test.cv_results_['mean_train_R2']}\\t{search_test.cv_results_['mean_test_R2']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_train_MSE']}\\t{search_test.cv_results_['mean_test_MSE']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_fit_time']*1e3}\\t{search_test.cv_results_['mean_score_time']*1e3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa1d87e-ccae-4a84-82a0-53a5991a68fc",
   "metadata": {},
   "source": [
    "Next, we optimize ESNs from PyESN using HAR as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2970df-2bbf-4514-a00b-e56c8de4ad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search = load(f'../results/pyesn_har_seq_esn_h{H}.joblib')\n",
    "except FileNotFoundError:\n",
    "    initial_esn_params = {'n_inputs': 3, 'n_outputs': 1, 'n_reservoir': 50,\n",
    "                          'spectral_radius': 0.0, 'sparsity': 0.5,\n",
    "                          'noise': 1e-3, 'input_shift': 0,\n",
    "                          'input_scaling': 0.4, 'teacher_forcing': True,\n",
    "                          'teacher_scaling': 1., 'teacher_shift': 0,\n",
    "                          'random_state': 42}\n",
    "    esn_pipeline = Pipeline(steps=[(\"har_features\", har_features),\n",
    "                                   (\"scaler\", MinMaxScaler()),\n",
    "                                   (\"esn\", TransformedTargetRegressor(\n",
    "                                       regressor=PyESN(**initial_esn_params),\n",
    "                                       transformer=MinMaxScaler()))\n",
    "                                  ])\n",
    "    # Run model selection\n",
    "    step1_params = {'esn__regressor__input_scaling': uniform(loc=1e-2, scale=1),\n",
    "                    'esn__regressor__spectral_radius': uniform(loc=0, scale=2)}\n",
    "    step2_params = {'esn__regressor__n_reservoir':\n",
    "                        [50, 100, 200, 400, 800, 1600, 3200, 6400],\n",
    "                    'esn__regressor__noise': loguniform(1e-5, 1e1)}\n",
    "    \n",
    "    kwargs_step1 = {'n_iter': 200, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step2 = {'n_iter': 200, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "\n",
    "    searches = [\n",
    "        ('step1', RandomizedSearchCV, step1_params, kwargs_step1),\n",
    "        ('step2', RandomizedSearchCV, step2_params, kwargs_step2)]\n",
    "    search = SequentialSearchCV(esn_pipeline, searches=searches).fit(X, y)\n",
    "    dump(search, f'../results/pyesn_har_seq_esn_h{H}.joblib')\n",
    "print(search.all_best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18658d10-e66c-48f4-a722-a442364e3b37",
   "metadata": {},
   "source": [
    "We do an additional GridSearchCV without parameters, because the object returns a mass of useful information.\n",
    "\n",
    "If we do it without parameters, simply the estimator will be fit on the train test splits defined in our cv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c2346b-c3e5-477a-b5c5-8efe680171cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search_test = load(f'../results/pyesn_har_seq_esn_h{H}_test.joblib')\n",
    "except FileNotFoundError:\n",
    "    search_test = GridSearchCV(estimator=clone(search.best_estimator_), param_grid={}, cv=ps_test,\n",
    "                               scoring=scoring, refit=\"R2\", return_train_score=True).fit(X, y)\n",
    "    dump(search_test, f'../results/pyesn_har_seq_esn_h{H}_test.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e9735a-32a1-43d9-be43-e5355df550f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"R^2 train\\tR^2 test\\tMSE train\\tMSE test\\tFit time in ms\\tScore time in ms\")\n",
    "print(f\"{search_test.cv_results_['mean_train_R2']}\\t{search_test.cv_results_['mean_test_R2']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_train_MSE']}\\t{search_test.cv_results_['mean_test_MSE']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_fit_time']*1e3}\\t{search_test.cv_results_['mean_score_time']*1e3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3702fe07-d633-4f14-a849-b8b693900af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(context=\"paper\", style=\"whitegrid\", font=\"serif\", font_scale=.8, rc={\"lines.linewidth\": 0.8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0587d289-0df2-4e3c-bd63-3d4a654b4e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "search1 = load(f\"results/pyesn_har_seq_esn_h{H}.joblib\")\n",
    "df1 = pd.DataFrame(search1.cv_results_).rename(\n",
    "    columns={\"param_esn__regressor__n_reservoir\": \"param_esn__regressor__hidden_layer_size\"})\n",
    "df1[\"Architecture\"] = \"PyESN\"\n",
    "\n",
    "search2 = load(f\"results/pyrcn_har_seq_esn_h{H}.joblib\")\n",
    "df2 = pd.DataFrame(search2.cv_results_)\n",
    "df2[\"Architecture\"] = \"PyRCN\"\n",
    "df = pd.concat([df1, df2]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2452931f-6b4e-4931-9c08-03e802558ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.mean(df1.groupby(by=\"param_esn__regressor__hidden_layer_size\").mean()[\"mean_score_time\"] / df2.groupby(by=\"param_esn__regressor__hidden_layer_size\").mean()[\"mean_score_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249ec57c-ee6b-4abe-8cba-b052ba705019",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.groupby(by=\"param_esn__regressor__hidden_layer_size\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc4ce60-5e15-4ec1-b2f5-fefb1b6acf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.groupby(by=\"param_esn__regressor__hidden_layer_size\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65589b0-2643-45ab-aaea-8b89b0bd9391",
   "metadata": {},
   "source": [
    "TODO: Update the mean speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a9d82e-f359-4d82-b66f-1fd901dddeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "fig.set_size_inches(4/1.6, 2.5/1.6)\n",
    "sns.pointplot(data=df, x=\"param_esn__regressor__hidden_layer_size\", y=\"mean_fit_time\", hue=\"Architecture\", linestyles=[\":\", \":\"], markers=\"X\", ci=\"sd\", ax=axs)\n",
    "sns.pointplot(data=df, x=\"param_esn__regressor__hidden_layer_size\", y=\"mean_score_time\", hue=\"Architecture\", ci=\"sd\", ax=axs)\n",
    "axs.legend(handles=axs.lines[::8+1], labels=[\"PyESN fit time\", \"PyESN score time\", \"PyRCN fit time\", \"PyRCN score time\"],\n",
    "           loc='upper left', bbox_to_anchor=(0.0, 1.0), ncol=1)\n",
    "axs.set_yscale('log')\n",
    "axs.set(xlabel=\"Reservoir size\", ylabel=\"Time in seconds\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig('ESN_Benchmark.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88fd9e1-33db-498a-b210-7f78896631dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "search1 = load(f\"results/skelm_har_seq_elm_h{H}.joblib\")\n",
    "df1 = pd.DataFrame(search1.cv_results_).rename(\n",
    "    columns={\"param_elm__regressor__n_neurons\": \"param_elm__regressor__hidden_layer_size\"})\n",
    "df1[\"Architecture\"] = \"HP-ELM\"\n",
    "\n",
    "search2 = load(f\"results/pyrcn_har_seq_elm_h{H}.joblib\")\n",
    "df2 = pd.DataFrame(search2.cv_results_)\n",
    "df2[\"Architecture\"] = \"PyRCN\"\n",
    "df = pd.concat([df1, df2]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285a8713-e999-44c2-b3a7-58f67245df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "fig.set_size_inches(4/1.6, 2.5/1.6)\n",
    "sns.pointplot(data=df, x=\"param_elm__regressor__hidden_layer_size\", y=\"mean_fit_time\", hue=\"Architecture\", linestyles=[\":\", \":\"], markers=\"X\", ci=\"sd\", ax=axs)\n",
    "sns.pointplot(data=df, x=\"param_elm__regressor__hidden_layer_size\", y=\"mean_score_time\", hue=\"Architecture\", ci=\"sd\", ax=axs)\n",
    "axs.legend(handles=axs.lines[::8+1], labels=[\"HP-ELM fit time\", \"HP-ELM score time\", \"PyRCN fit time\", \"PyRCN score time\"],\n",
    "           loc='upper left', bbox_to_anchor=(0.0, 1.0), ncol=1)\n",
    "axs.set_yscale('log')\n",
    "axs.set(xlabel=\"Reservoir size\", ylabel=\"Time in seconds\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig('ELM_Benchmark.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a95ec53-5d6e-4056-a276-2c426d47b21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.sort_values(by=\"param_elm__regressor__hidden_layer_size\").reset_index(drop=True)\n",
    "df2 = df2.sort_values(by=\"param_elm__regressor__hidden_layer_size\").reset_index(drop=True)\n",
    "df1[\"fit_speedup\"] = df1[\"mean_fit_time\"] / df2[\"mean_fit_time\"]\n",
    "df1[\"score_speedup\"] = df1[\"mean_score_time\"] / df2[\"mean_score_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e154ef34-9a49-4c09-828e-5421f5e1326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1[\"params\"][-1:], df2[\"params\"][-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48a7279-288f-40e5-baf4-2138da66ad64",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "# fig.set_size_inches(4/1.6, 2.5/1.6)\n",
    "sns.pointplot(data=df1, x=\"param_elm__regressor__hidden_layer_size\", y=\"fit_speedup\", linestyles=\":\", markers=\"X\", ci=\"sd\", ax=axs)\n",
    "sns.pointplot(data=df1, x=\"param_elm__regressor__hidden_layer_size\", y=\"score_speedup\", ci=\"sd\", ax=axs)\n",
    "axs.legend(handles=axs.lines[::8+1], labels=[\"fit time\", \"PyESN score time\", \"PyRCN fit time\", \"PyRCN score time\"],\n",
    "           loc='upper left', bbox_to_anchor=(0.0, 1.0), ncol=1)\n",
    "axs.set_yscale('log')\n",
    "axs.set(xlabel=\"Reservoir size\", ylabel=\"Time in seconds\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig('ESN_speedup.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a653ea-cf88-4856-8123-1f027bb3772a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.mean(df1[\"fit_speedup\"][:31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523a2adc-2477-4698-b872-6efd6fafd988",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[[\"param_elm__regressor__hidden_layer_size\", \"mean_fit_time\", \"mean_score_time\"]][-31:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3847cebf-1122-4f2b-b348-20b2ca2c9f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[[\"param_elm__regressor__hidden_layer_size\", \"mean_fit_time\", \"mean_score_time\"]][-31:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e14f3d0-49df-499c-864a-e362bad276fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
