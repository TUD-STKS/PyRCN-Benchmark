{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "033f4152-7dff-4ce3-a0da-26dee5aa3e45",
   "metadata": {},
   "source": [
    "# Benchmark PyRCN\n",
    "\n",
    "This notebook accompanies the publication \"PyRCN: A Toolbox for Exploration and Application of Reservoir Computing Networks\" and serves as a benchmark test for PyRCN and other libraries.\n",
    "\n",
    "Therefore, we reproduce parts of the publication [[1]](#1), where the stock price return volatility was predicted using Echo State Networks.\n",
    "\n",
    "We compare the performance of different RCN architectures included in PyRCN with the toolboxes [PyESN](https://github.com/cknd/pyESN) and HP-ELM [[2]](#2) using the [scikit-ELM](https://github.com/akusok/scikit-elm) (skELM) as the interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e078dadd-e54c-42e2-a2a4-786534e52b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import MinMaxScaler, FunctionTransformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.utils.fixes import loguniform\n",
    "from scipy.stats import uniform\n",
    "from joblib import dump, load\n",
    "\n",
    "from pyrcn.extreme_learning_machine import ELMRegressor\n",
    "from pyrcn.echo_state_network import ESNRegressor\n",
    "from pyrcn.model_selection import SequentialSearchCV\n",
    "from skelm import ELMRegressor as SkELMRegressor\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from preprocessing import ts2super, compute_average_volatility\n",
    "from model_selection import PredefinedTrainValidationTestSplit\n",
    "from adapter import PyESN\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('png', 'pdf')\n",
    "sns.set_theme(context=\"paper\", style=\"whitegrid\", font=\"serif\", font_scale=.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab7fa07-7dab-412d-b044-de201098c590",
   "metadata": {},
   "source": [
    "## Prepare all datasets\n",
    "\n",
    "We used the datasets provided by [[1]](#1), namely the stock price volatilities for the three Nasdaq companies Caterpillar (CAT), eBay (EBAY) and Microsoft (MSFT), with each datasets having a length of 2745 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d682b5dc-6632-4184-8dac-0f0fe2f42ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = OrderedDict({\"CAT\": 0, \"EBAY\": 1, \"MSFT\": 2})\n",
    "data = [None] * len(datasets)\n",
    "\n",
    "for dataset, k in datasets.items():\n",
    "    data[k] = pd.read_csv(f\"../data/{dataset}.csv\")[\"t0\"].to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532cb2f1-f95d-41be-8ee8-54df39675d3b",
   "metadata": {},
   "source": [
    "The input data was the original time series and the output data the time series shifted by 1, 5 and 21 days, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f1b8ce-184e-49e5-bbc9-e45c0c47b40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "H=21  # pre-trained models available for 1, 5, 21\n",
    "\n",
    "# ts2super simply returns a dataframe containing the original and shifted by H time series.\n",
    "df = pd.concat([ts2super(d, 0, H) for d in data])\n",
    "X = df.iloc[:, 0].values.reshape(-1, 1)\n",
    "y = df.iloc[:, -1].values.reshape(-1, 1)\n",
    "\n",
    "df_input = df.iloc[:, 0].to_frame()\n",
    "df_input[\"t5\"] = df_input.t0.rolling(window=5, min_periods=1).mean()\n",
    "df_input[\"t22\"] = df_input.t0.rolling(window=22, min_periods=1).mean()\n",
    "df_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b53c906-7f4a-4fcc-965a-5a22a0d8af84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input_extended = pd.concat([pd.DataFrame(data={\"t0\": [0], \"t5\": [0], \"t22\": [0]}), df_input], ignore_index=False).reset_index(drop=True)\n",
    "df_input_extended[\"t0\"] = df_input_extended[\"t0\"].shift(periods=-1)\n",
    "df_input_extended = df_input_extended.dropna()\n",
    "df_input_extended"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8de015-8681-406f-acaa-f367185dc536",
   "metadata": {},
   "source": [
    "Provide a custom train test split for this task, and all scores that we aim to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99e7969-b5f9-429d-910f-3c10026413e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fold = [[k] * len(ts2super(d, 0, H)) for k, d in enumerate(data)]\n",
    "test_fold = list(itertools.chain.from_iterable(test_fold))\n",
    "\n",
    "ps = PredefinedTrainValidationTestSplit(test_fold=test_fold, validation=True)\n",
    "ps_test = PredefinedTrainValidationTestSplit(test_fold=test_fold, validation=False)\n",
    "\n",
    "scoring = {\"MSE\": \"neg_mean_squared_error\", \"RMSE\": \"neg_root_mean_squared_error\", \"R2\": \"r2\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532455c7-0d67-4df7-9565-697a66b9aa06",
   "metadata": {},
   "source": [
    "## HAR\n",
    "\n",
    "As in [[1]](#1), we compared two different input data: The original time series and the Heterogeneous Autoregressive (HAR) feature set [[3]](#3), which expands the one-dimensional time series to three dimensions by adding moving-average filtered representations with filter lengths of five and 22 days, respectively. Since the description of the HAR feature set by [[3]](#3), Equations (4)â€“(7)), and [[1]](#1) have different temporal contexts for the moving average filter, we compared both implementations. We use the description by [[3]](#3) in the following, because the results were comparable to [[1]](#1), even though not equal.\n",
    "\n",
    "Each input and target time series was separately scaled to the interval [0, 1] before using it to train and test models. The constants therefore were computed on the training dataset of each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d762ca99-a2f3-4be4-b749-f66a8978dc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_volatility_transformer = FunctionTransformer(\n",
    "    func=compute_average_volatility, kw_args={\"window_length\": 1})\n",
    "week_volatility_transformer = FunctionTransformer(\n",
    "    func=compute_average_volatility, kw_args={\"window_length\": 5})\n",
    "month_volatility_transformer = FunctionTransformer(\n",
    "    func=compute_average_volatility, kw_args={\"window_length\": 22})\n",
    "har_features = FeatureUnion(\n",
    "    transformer_list=[(\"day\", day_volatility_transformer),\n",
    "                      (\"week\", week_volatility_transformer),\n",
    "                      (\"month\", month_volatility_transformer)])\n",
    "har_pipeline = Pipeline(\n",
    "    steps=[(\"har_features\", har_features),\n",
    "           (\"scaler\", MinMaxScaler()),\n",
    "           (\"lstsq\", TransformedTargetRegressor(transformer=MinMaxScaler()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a8f2c3-1288-474a-bb98-4a447ee27e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search = load(f'../results/har_grid_h{H}.joblib')\n",
    "except FileNotFoundError:\n",
    "    search = GridSearchCV(estimator=har_pipeline, param_grid={}, cv=ps_test,\n",
    "                          scoring=scoring, refit=\"R2\", return_train_score=True,\n",
    "                          verbose=10).fit(X, y)\n",
    "    dump(search, f'../results/har_grid_h{H}.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd8a0e4-bfb6-4801-8c3f-892eed00f9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"R^2 train\\tR^2 test\\tMSE train\\tMSE test\\tFit time in ms\\tScore time in ms\")\n",
    "print(f\"{search.cv_results_['mean_train_R2']}\\t{search.cv_results_['mean_test_R2']}\\t\"\n",
    "      f\"{search.cv_results_['mean_train_MSE']}\\t{search.cv_results_['mean_test_MSE']}\\t\"\n",
    "      f\"{search.cv_results_['mean_fit_time']*1e3}\\t{search.cv_results_['mean_score_time']*1e3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e6d329-f1fc-426e-b1c3-c2ab73e76cde",
   "metadata": {},
   "source": [
    "## PyRCN ESN\n",
    "\n",
    "Here, we optimize ESNs from PyRCN using only the original time series as features. During the optimization, we only use the training and the validation sets. The test set remains untouched.\n",
    "\n",
    "In contrast to [[1]](#1), we did not jointly optimize all hyper-parameters but used our proposed sequential optimization. We aimed to jointly optimize input scaling $\\alpha_{\\mathrm{u}}$ and spectral radius $\\rho$, then leakage $\\lambda$, bias scaling $\\alpha_{\\mathrm{bi}}$, and, finally, jointly the regularization parameter $\\beta$ and the number of neurons $N_\\mathrm{res}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da7966a-bd89-4135-b0f2-eb39222bef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search = load(f'../results/pyrcn_seq_esn_h{H}.joblib')\n",
    "except FileNotFoundError:\n",
    "    initial_esn_params = {\n",
    "        'hidden_layer_size': 50, 'k_in': 1, 'input_scaling': 0.4,\n",
    "        'input_activation': 'identity', 'bias_scaling': 0.0,\n",
    "        'spectral_radius': 0.0, 'leakage': 1.0, 'k_rec': 10,\n",
    "        'reservoir_activation': 'tanh', 'bidirectional': False,\n",
    "        'alpha': 1e-5, 'random_state': 42}\n",
    "    esn_pipeline = Pipeline(steps=[(\"scaler\", MinMaxScaler()),\n",
    "                                   (\"esn\", TransformedTargetRegressor(\n",
    "                                       regressor=ESNRegressor(**initial_esn_params),\n",
    "                                       transformer=MinMaxScaler()))\n",
    "                                  ])\n",
    "    # Run model selection\n",
    "    step1_params = {'esn__regressor__input_scaling': uniform(loc=1e-2, scale=1),\n",
    "                    'esn__regressor__spectral_radius': uniform(loc=0, scale=2)}\n",
    "    step2_params = {'esn__regressor__leakage': uniform(1e-5, 1e0)}\n",
    "    step3_params = {'esn__regressor__bias_scaling': uniform(loc=0, scale=3)}\n",
    "    step4_params = {'esn__regressor__hidden_layer_size':\n",
    "                        [50, 100, 200, 400, 800, 1600, 3200, 6400],\n",
    "                    'esn__regressor__alpha': loguniform(1e-5, 1e1)}\n",
    "    \n",
    "    kwargs_step1 = {'n_iter': 200, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step2 = {'n_iter': 50, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step3 = {'n_iter': 50, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step4 = {'n_iter': 200, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "\n",
    "    searches = [\n",
    "        ('step1', RandomizedSearchCV, step1_params, kwargs_step1),\n",
    "        ('step2', RandomizedSearchCV, step2_params, kwargs_step2),\n",
    "        ('step3', RandomizedSearchCV, step3_params, kwargs_step3),\n",
    "        ('step4', RandomizedSearchCV, step4_params, kwargs_step4)]\n",
    "    search = SequentialSearchCV(esn_pipeline, searches=searches).fit(X, y)\n",
    "    dump(search, f'../results/pyrcn_seq_esn_h{H}.joblib')\n",
    "print(search.all_best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b4d1c7-d214-4699-ad2c-2766f0c850f7",
   "metadata": {},
   "source": [
    "We do an additional GridSearchCV without parameters for the final evaluation on the test set, because the object returns a mass of useful information.\n",
    "\n",
    "In this step, no parameters are optimized anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f5e590-431a-4b17-8729-2b979b09af19",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search_test = load(f'../results/pyrcn_seq_esn_h{H}_test.joblib')\n",
    "except FileNotFoundError:\n",
    "    search_test = GridSearchCV(estimator=clone(search.best_estimator_), param_grid={}, cv=ps_test,\n",
    "                               scoring=scoring, refit=\"R2\", return_train_score=True).fit(X, y)\n",
    "    dump(search_test, f'../results/pyrcn_seq_esn_h{H}_test.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4a1be4-7a80-4d7a-a365-c8252cb2b4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"R^2 train\\tR^2 test\\tMSE train\\tMSE test\\tFit time in ms\\tScore time in ms\")\n",
    "print(f\"{search_test.cv_results_['mean_train_R2']}\\t{search_test.cv_results_['mean_test_R2']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_train_MSE']}\\t{search_test.cv_results_['mean_test_MSE']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_fit_time']*1e3}\\t{search_test.cv_results_['mean_score_time']*1e3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fcd07f-411c-4022-be8a-6081cb9ab3f6",
   "metadata": {},
   "source": [
    "Next, we optimize ESNs from PyRCN using HAR as features.\n",
    "\n",
    "In contrast to [[1]](#1), we did not jointly optimize all hyper-parameters but used our proposed sequential optimization. We aimed to jointly optimize input scaling $\\alpha_{\\mathrm{u}}$ and spectral radius $\\rho$, then leakage $\\lambda$, bias scaling $\\alpha_{\\mathrm{bi}}$, and, finally, jointly the regularization parameter $\\beta$ and the number of neurons $N_\\mathrm{res}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c42717-100d-4e5b-bae3-59334ac67a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search = load(f'../results/pyrcn_har_seq_esn_h{H}.joblib')\n",
    "except FileNotFoundError:\n",
    "    initial_esn_params = {\n",
    "        'hidden_layer_size': 50, 'k_in': 1, 'input_scaling': 0.4,\n",
    "        'input_activation': 'identity', 'bias_scaling': 0.0,\n",
    "        'spectral_radius': 0.0, 'leakage': 1.0, 'k_rec': 10,\n",
    "        'reservoir_activation': 'tanh', 'bidirectional': False,\n",
    "        'alpha': 1e-5, 'random_state': 42}\n",
    "    esn_pipeline = Pipeline(steps=[(\"har_features\", har_features),\n",
    "                                   (\"scaler\", MinMaxScaler()),\n",
    "                                   (\"esn\", TransformedTargetRegressor(\n",
    "                                       regressor=ESNRegressor(**initial_esn_params),\n",
    "                                       transformer=MinMaxScaler()))\n",
    "                                  ])\n",
    "    # Run model selection\n",
    "    step1_params = {'esn__regressor__input_scaling': uniform(loc=1e-2, scale=1),\n",
    "                    'esn__regressor__spectral_radius': uniform(loc=0, scale=2)}\n",
    "    step2_params = {'esn__regressor__leakage': uniform(1e-5, 1e0)}\n",
    "    step3_params = {'esn__regressor__bias_scaling': uniform(loc=0, scale=3)}\n",
    "    step4_params = {'esn__regressor__hidden_layer_size':\n",
    "                        [50, 100, 200, 400, 800, 1600, 3200, 6400],\n",
    "                    'esn__regressor__alpha': loguniform(1e-5, 1e1)}\n",
    "    \n",
    "    kwargs_step1 = {'n_iter': 200, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step2 = {'n_iter': 50, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step3 = {'n_iter': 50, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step4 = {'n_iter': 200, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "\n",
    "    searches = [\n",
    "        ('step1', RandomizedSearchCV, step1_params, kwargs_step1),\n",
    "        ('step2', RandomizedSearchCV, step2_params, kwargs_step2),\n",
    "        ('step3', RandomizedSearchCV, step3_params, kwargs_step3),\n",
    "        ('step4', RandomizedSearchCV, step4_params, kwargs_step4)]\n",
    "    search = SequentialSearchCV(esn_pipeline, searches=searches).fit(X, y)\n",
    "    dump(search, f'../results/pyrcn_har_seq_esn_h{H}.joblib')\n",
    "print(search.all_best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a10d2e-9b39-4dcd-9e1c-aec94209af8c",
   "metadata": {},
   "source": [
    "We do an additional GridSearchCV without parameters for the final evaluation on the test set, because the object returns a mass of useful information.\n",
    "\n",
    "In this step, no parameters are optimized anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fcf3f0-ba5f-4b38-b44e-59709fc123f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search_test = load(f'../results/pyrcn_har_seq_esn_h{H}_test.joblib')\n",
    "except FileNotFoundError:\n",
    "    search_test = GridSearchCV(estimator=clone(search.best_estimator_), param_grid={}, cv=ps_test,\n",
    "                               scoring=scoring, refit=\"R2\", return_train_score=True).fit(X, y)\n",
    "    dump(search_test, f'../results/pyrcn_har_seq_esn_h{H}_test.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83ec0d2-c450-44aa-86ee-e473d0520e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"R^2 train\\tR^2 test\\tMSE train\\tMSE test\\tFit time in ms\\tScore time in ms\")\n",
    "print(f\"{search_test.cv_results_['mean_train_R2']}\\t{search_test.cv_results_['mean_test_R2']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_train_MSE']}\\t{search_test.cv_results_['mean_test_MSE']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_fit_time']*1e3}\\t{search_test.cv_results_['mean_score_time']*1e3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c42c68-5e1a-455e-94a8-d16dd375c6fa",
   "metadata": {},
   "source": [
    "## PyRCN ELM\n",
    "\n",
    "Next, we optimize ELMs from PyRCN using only the original time series as features.\n",
    "\n",
    "In contrast to [[1]](#1), we did not jointly optimize all hyper-parameters but used our proposed sequential optimization. We aimed to optimize input scaling $\\alpha_{\\mathrm{u}}$, then bias scaling $\\alpha_{\\mathrm{bi}}$, and, finally, jointly the regularization parameter $\\beta$ and the number of neurons $N_\\mathrm{res}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475b9669-9977-4030-ace0-868442cf2386",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search = load(f'../results/pyrcn_seq_elm_h{H}.joblib')\n",
    "except FileNotFoundError:\n",
    "    initial_elm_params = {\n",
    "        'hidden_layer_size': 50, 'k_in': 1, 'input_scaling': 0.4,\n",
    "        'input_activation': 'tanh', 'bias_scaling': 0.0,\n",
    "        'alpha': 1e-5, 'random_state': 42}\n",
    "    elm_pipeline = Pipeline(steps=[(\"scaler\", MinMaxScaler()),\n",
    "                                   (\"elm\", TransformedTargetRegressor(\n",
    "                                       regressor=ELMRegressor(**initial_elm_params),\n",
    "                                       transformer=MinMaxScaler()))\n",
    "                                  ])\n",
    "    # Run model selection\n",
    "    step1_params = {'elm__regressor__input_scaling': uniform(loc=1e-2, scale=1)}\n",
    "    step2_params = {'elm__regressor__bias_scaling': uniform(loc=0, scale=3)}\n",
    "    step3_params = {'elm__regressor__hidden_layer_size':\n",
    "                        [50, 100, 200, 400, 800, 1600, 3200, 6400],\n",
    "                    'elm__regressor__alpha': loguniform(1e-5, 1e1)}\n",
    "    \n",
    "    kwargs_step1 = {'n_iter': 50, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step2 = {'n_iter': 50, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step3 = {'n_iter': 200, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "\n",
    "    searches = [\n",
    "        ('step1', RandomizedSearchCV, step1_params, kwargs_step1),\n",
    "        ('step2', RandomizedSearchCV, step2_params, kwargs_step2),\n",
    "        ('step3', RandomizedSearchCV, step3_params, kwargs_step3)]\n",
    "    search = SequentialSearchCV(elm_pipeline, searches=searches).fit(X, y)\n",
    "    dump(search, f'../results/pyrcn_seq_elm_h{H}.joblib')\n",
    "print(search.all_best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e0f5d6-738d-4cbe-8328-c0d1a1c04af2",
   "metadata": {},
   "source": [
    "We do an additional GridSearchCV without parameters for the final evaluation on the test set, because the object returns a mass of useful information.\n",
    "\n",
    "In this step, no parameters are optimized anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51c5c19-14e5-4d42-880a-0e48b062e496",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search_test = load(f'../results/pyrcn_seq_elm_h{H}_test.joblib')\n",
    "except FileNotFoundError:\n",
    "    search_test = GridSearchCV(estimator=clone(search.best_estimator_), param_grid={}, cv=ps_test,\n",
    "                               scoring=scoring, refit=\"R2\", return_train_score=True).fit(X, y)\n",
    "    dump(search_test, f'../results/pyrcn_seq_elm_h{H}_test.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8c90d1-c9c9-4ee3-a356-2cd003e271a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"R^2 train\\tR^2 test\\tMSE train\\tMSE test\\tFit time in ms\\tScore time in ms\")\n",
    "print(f\"{search_test.cv_results_['mean_train_R2']}\\t{search_test.cv_results_['mean_test_R2']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_train_MSE']}\\t{search_test.cv_results_['mean_test_MSE']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_fit_time']*1e3}\\t{search_test.cv_results_['mean_score_time']*1e3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e59891-409d-4979-bab2-6a017fb0ff9f",
   "metadata": {},
   "source": [
    "Next, we optimize ELMs from PyRCN using HAR as features.\n",
    "\n",
    "In contrast to [[1]](#1), we did not jointly optimize all hyper-parameters but used our proposed sequential optimization. We aimed to optimize input scaling $\\alpha_{\\mathrm{u}}$, then bias scaling $\\alpha_{\\mathrm{bi}}$, and, finally, jointly the regularization parameter $\\beta$ and the number of neurons $N_\\mathrm{res}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac485279-fccf-47e8-9bf7-1570101fd29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search = load(f'../results/pyrcn_har_seq_elm_h{H}.joblib')\n",
    "except FileNotFoundError:\n",
    "    initial_elm_params = {\n",
    "        'hidden_layer_size': 50, 'k_in': 1, 'input_scaling': 0.4,\n",
    "        'input_activation': 'tanh', 'bias_scaling': 0.0,\n",
    "        'alpha': 1e-5, 'random_state': 42}\n",
    "    elm_pipeline = Pipeline(steps=[(\"har_features\", har_features),\n",
    "                                   (\"scaler\", MinMaxScaler()),\n",
    "                                   (\"elm\", TransformedTargetRegressor(\n",
    "                                       regressor=ELMRegressor(**initial_elm_params),\n",
    "                                       transformer=MinMaxScaler()))\n",
    "                                  ])\n",
    "    # Run model selection\n",
    "    step1_params = {'elm__regressor__input_scaling': uniform(loc=1e-2, scale=1)}\n",
    "    step2_params = {'elm__regressor__bias_scaling': uniform(loc=0, scale=3)}\n",
    "    step3_params = {'elm__regressor__hidden_layer_size':\n",
    "                        [50, 100, 200, 400, 800, 1600, 3200, 6400],\n",
    "                    'elm__regressor__alpha': loguniform(1e-5, 1e1)}\n",
    "    \n",
    "    kwargs_step1 = {'n_iter': 50, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step2 = {'n_iter': 50, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step3 = {'n_iter': 200, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "\n",
    "    searches = [\n",
    "        ('step1', RandomizedSearchCV, step1_params, kwargs_step1),\n",
    "        ('step2', RandomizedSearchCV, step2_params, kwargs_step2),\n",
    "        ('step3', RandomizedSearchCV, step3_params, kwargs_step3)]\n",
    "    search = SequentialSearchCV(elm_pipeline, searches=searches).fit(X, y)\n",
    "    dump(search, f'../results/pyrcn_har_seq_elm_h{H}.joblib')\n",
    "print(search.all_best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa97d242-c8a2-429a-b4ea-b78a77b5cf78",
   "metadata": {},
   "source": [
    "We do an additional GridSearchCV without parameters for the final evaluation on the test set, because the object returns a mass of useful information.\n",
    "\n",
    "In this step, no parameters are optimized anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d63bda-06ac-4155-b097-50ddf1a70488",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search_test = load(f'../results/pyrcn_har_seq_elm_h{H}_test.joblib')\n",
    "except FileNotFoundError:\n",
    "    search_test = GridSearchCV(estimator=clone(search.best_estimator_), param_grid={}, cv=ps_test,\n",
    "                               scoring=scoring, refit=\"R2\", return_train_score=True).fit(X, y)\n",
    "    dump(search_test, f'../results/pyrcn_har_seq_elm_h{H}_test.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c6b0a2-1660-4361-a9e5-2121108dda94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"R^2 train\\tR^2 test\\tMSE train\\tMSE test\\tFit time in ms\\tScore time in ms\")\n",
    "print(f\"{search_test.cv_results_['mean_train_R2']}\\t{search_test.cv_results_['mean_test_R2']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_train_MSE']}\\t{search_test.cv_results_['mean_test_MSE']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_fit_time']*1e3}\\t{search_test.cv_results_['mean_score_time']*1e3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160dc009-3ec4-4eba-8ca6-3cc64b4430bc",
   "metadata": {},
   "source": [
    "## HP-ELM\n",
    "\n",
    "Next, we optimize ELMs from HP-ELM using only the original time series as features.\n",
    "\n",
    "In contrast to [[1]](#1), we did not jointly optimize all hyper-parameters but used our proposed sequential optimization. Note that the HP-ELM does not have the hyper-parameters input scaling $\\alpha_{\\mathrm{u}}$ and bias scaling $\\alpha_{\\mathrm{bi}}$. Hence we only jointly optimize the regularization parameter $\\beta$ and the number of neurons $N_\\mathrm{res}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eb0b5a-a034-4d69-be5e-3bc41efab22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search = load(f'../results/skelm_seq_elm_h{H}.joblib')\n",
    "except FileNotFoundError:\n",
    "    initial_elm_params = {'n_neurons': 50, 'density': 0.1, \n",
    "                          'ufunc': 'tanh', 'alpha': 1e-5,\n",
    "                          'random_state': 42}\n",
    "    elm_pipeline = Pipeline(steps=[(\"scaler\", MinMaxScaler()),\n",
    "                                   (\"elm\", TransformedTargetRegressor(\n",
    "                                       regressor=SkELMRegressor(**initial_elm_params),\n",
    "                                       transformer=MinMaxScaler()))\n",
    "                                  ])\n",
    "    # Run model selection\n",
    "    step1_params = {'elm__regressor__n_neurons':\n",
    "                        [50, 100, 200, 400, 800, 1600, 3200, 6400],\n",
    "                    'elm__regressor__alpha': loguniform(1e-5, 1e1)}\n",
    "    \n",
    "    kwargs_step1 = {'n_iter': 200, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "\n",
    "    searches = [\n",
    "        ('step1', RandomizedSearchCV, step1_params, kwargs_step1)]\n",
    "    search = SequentialSearchCV(elm_pipeline, searches=searches).fit(X, y)\n",
    "    dump(search, f'../results/skelm_seq_elm_h{H}.joblib')\n",
    "print(search.all_best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd523fb1-d360-4b72-a640-07cacc4831da",
   "metadata": {},
   "source": [
    "We do an additional GridSearchCV without parameters for the final evaluation on the test set, because the object returns a mass of useful information.\n",
    "\n",
    "In this step, no parameters are optimized anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a60403-de08-4d60-9b48-971ecbbacef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search_test = load(f'../results/skelm_seq_elm_h{H}_test.joblib')\n",
    "except FileNotFoundError:\n",
    "    search_test = GridSearchCV(estimator=clone(search.best_estimator_), param_grid={}, cv=ps_test,\n",
    "                               scoring=scoring, refit=\"R2\", return_train_score=True).fit(X, y)\n",
    "    dump(search_test, f'../results/skelm_seq_elm_h{H}_test.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f62125-6690-42f6-9668-055ffe3d6a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"R^2 train\\tR^2 test\\tMSE train\\tMSE test\\tFit time in ms\\tScore time in ms\")\n",
    "print(f\"{search_test.cv_results_['mean_train_R2']}\\t{search_test.cv_results_['mean_test_R2']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_train_MSE']}\\t{search_test.cv_results_['mean_test_MSE']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_fit_time']*1e3}\\t{search_test.cv_results_['mean_score_time']*1e3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f045a5e3-ed10-4f0c-b2bc-e21edab91e4c",
   "metadata": {},
   "source": [
    "Next, we optimize ELMs from HP-ELM using HAR as features.\n",
    "\n",
    "In contrast to [[1]](#1), we did not jointly optimize all hyper-parameters but used our proposed sequential optimization. Note that the HP-ELM does not have the hyper-parameters input scaling $\\alpha_{\\mathrm{u}}$ and bias scaling $\\alpha_{\\mathrm{bi}}$. Hence we only jointly optimize the regularization parameter $\\beta$ and the number of neurons $N_\\mathrm{res}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a14f20-03fd-4e90-a31f-467cbba3ac47",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search = load(f'../results/skelm_har_seq_elm_h{H}.joblib')\n",
    "except FileNotFoundError:\n",
    "    initial_elm_params = {'n_neurons': 200, 'density': 0.1,\n",
    "                          'ufunc': 'tanh', 'alpha': 1e-5,\n",
    "                          'random_state': 42}\n",
    "    elm_pipeline = Pipeline(steps=[(\"har_features\", har_features),\n",
    "                                   (\"scaler\", MinMaxScaler()),\n",
    "                                   (\"elm\", TransformedTargetRegressor(\n",
    "                                       regressor=SkELMRegressor(**initial_elm_params),\n",
    "                                       transformer=MinMaxScaler()))\n",
    "                                  ])\n",
    "    # Run model selection\n",
    "    step1_params = {'elm__regressor__n_neurons':\n",
    "                        [50, 100, 200, 400, 800, 1600, 3200, 6400],\n",
    "                    'elm__regressor__alpha': loguniform(1e-5, 1e1)}\n",
    "    \n",
    "    kwargs_step1 = {'n_iter': 200, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "\n",
    "    searches = [\n",
    "        ('step1', RandomizedSearchCV, step1_params, kwargs_step1)]\n",
    "    search = SequentialSearchCV(elm_pipeline, searches=searches).fit(X, y)\n",
    "    dump(search, f'../results/skelm_har_seq_elm_h{H}.joblib')\n",
    "print(search.all_best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e55b881-821b-476f-9a13-3740dbd9bc51",
   "metadata": {},
   "source": [
    "We do an additional GridSearchCV without parameters for the final evaluation on the test set, because the object returns a mass of useful information.\n",
    "\n",
    "In this step, no parameters are optimized anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5305b0-77e0-4371-906f-7add1e3d6ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search_test = load(f'../results/skelm_har_seq_elm_h{H}_test.joblib')\n",
    "except FileNotFoundError:\n",
    "    search_test = GridSearchCV(estimator=clone(search.best_estimator_), param_grid={}, cv=ps_test,\n",
    "                               scoring=scoring, refit=\"R2\", return_train_score=True).fit(X, y)\n",
    "    dump(search_test, f'../results/skelm_har_seq_elm_h{H}_test.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858af3b1-a9c5-4598-8404-39c67036b7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"R^2 train\\tR^2 test\\tMSE train\\tMSE test\\tFit time in ms\\tScore time in ms\")\n",
    "print(f\"{search_test.cv_results_['mean_train_R2']}\\t{search_test.cv_results_['mean_test_R2']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_train_MSE']}\\t{search_test.cv_results_['mean_test_MSE']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_fit_time']*1e3}\\t{search_test.cv_results_['mean_score_time']*1e3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474a7e30-3e2f-4afe-acb9-7226fc130324",
   "metadata": {},
   "source": [
    "## PyESN ESN\n",
    "\n",
    "Next, we optimize ESNs from PyESN using only the original time series as features.\n",
    "\n",
    "In contrast to [[1]](#1), we did not jointly optimize all hyper-parameters but used our proposed sequential optimization. Note that PyESN does not have the hyper-parameters bias scaling $\\alpha_{\\mathrm{bi}}$ and leakage $\\lambda$. Hence, we only jointly optimized input scaling $\\alpha_{\\mathrm{u}}$ and spectral radius $\\rho$, and, finally, jointly the regularization parameter $\\beta$ and the number of neurons $N_\\mathrm{res}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1b22cb-39cd-483a-bb32-9ea4f44f5347",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search = load(f'../results/pyesn_seq_esn_h{H}.joblib')\n",
    "except FileNotFoundError:\n",
    "    initial_esn_params = {'n_inputs': 1, 'n_outputs': 1, 'n_reservoir': 50,\n",
    "                          'spectral_radius': 0.0, 'sparsity': 0.5,\n",
    "                          'noise': 1e-3, 'input_shift': 0,\n",
    "                          'input_scaling': 0.4, 'teacher_forcing': True,\n",
    "                          'teacher_scaling': 1., 'teacher_shift': 0,\n",
    "                          'random_state': 42}\n",
    "    esn_pipeline = Pipeline(steps=[(\"scaler\", MinMaxScaler()),\n",
    "                                   (\"esn\", TransformedTargetRegressor(\n",
    "                                       regressor=PyESN(**initial_esn_params),\n",
    "                                       transformer=MinMaxScaler()))\n",
    "                                  ])\n",
    "    # Run model selection\n",
    "    step1_params = {'esn__regressor__input_scaling': uniform(loc=1e-2, scale=1),\n",
    "                    'esn__regressor__spectral_radius': uniform(loc=0, scale=2)}\n",
    "    step2_params = {'esn__regressor__n_reservoir':\n",
    "                        [50, 100, 200, 400, 800, 1600, 3200, 6400],\n",
    "                    'esn__regressor__noise': loguniform(1e-5, 1e1)}\n",
    "    \n",
    "    kwargs_step1 = {'n_iter': 200, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step2 = {'n_iter': 200, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "\n",
    "    searches = [\n",
    "        ('step1', RandomizedSearchCV, step1_params, kwargs_step1),\n",
    "        ('step2', RandomizedSearchCV, step2_params, kwargs_step2)]\n",
    "    search = SequentialSearchCV(esn_pipeline, searches=searches).fit(X, y)\n",
    "    dump(search, f'../results/pyesn_seq_esn_h{H}.joblib')\n",
    "print(search.all_best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d514d1-8686-46db-bfaf-9ad21c4682d4",
   "metadata": {},
   "source": [
    "We do an additional GridSearchCV without parameters for the final evaluation on the test set, because the object returns a mass of useful information.\n",
    "\n",
    "In this step, no parameters are optimized anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2044c0-f435-4e1f-8658-49422d96ebca",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search_test = load(f'../results/pyesn_seq_esn_h{H}_test.joblib')\n",
    "except FileNotFoundError:\n",
    "    search_test = GridSearchCV(estimator=clone(search.best_estimator_), param_grid={}, cv=ps_test,\n",
    "                               scoring=scoring, refit=\"R2\", return_train_score=True).fit(X, y)\n",
    "    dump(search_test, f'../results/pyesn_seq_esn_h{H}_test.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378af756-3f32-4133-9121-8ec7e17a7ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"R^2 train\\tR^2 test\\tMSE train\\tMSE test\\tFit time in ms\\tScore time in ms\")\n",
    "print(f\"{search_test.cv_results_['mean_train_R2']}\\t{search_test.cv_results_['mean_test_R2']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_train_MSE']}\\t{search_test.cv_results_['mean_test_MSE']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_fit_time']*1e3}\\t{search_test.cv_results_['mean_score_time']*1e3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa1d87e-ccae-4a84-82a0-53a5991a68fc",
   "metadata": {},
   "source": [
    "Next, we optimize ESNs from PyESN using HAR as features.\n",
    "\n",
    "In contrast to [[1]](#1), we did not jointly optimize all hyper-parameters but used our proposed sequential optimization. Note that PyESN does not have the hyper-parameters bias scaling $\\alpha_{\\mathrm{bi}}$ and leakage $\\lambda$. Hence, we only jointly optimized input scaling $\\alpha_{\\mathrm{u}}$ and spectral radius $\\rho$, and, finally, jointly the regularization parameter $\\beta$ and the number of neurons $N_\\mathrm{res}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2970df-2bbf-4514-a00b-e56c8de4ad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search = load(f'../results/pyesn_har_seq_esn_h{H}.joblib')\n",
    "except FileNotFoundError:\n",
    "    initial_esn_params = {'n_inputs': 3, 'n_outputs': 1, 'n_reservoir': 50,\n",
    "                          'spectral_radius': 0.0, 'sparsity': 0.5,\n",
    "                          'noise': 1e-3, 'input_shift': 0,\n",
    "                          'input_scaling': 0.4, 'teacher_forcing': True,\n",
    "                          'teacher_scaling': 1., 'teacher_shift': 0,\n",
    "                          'random_state': 42}\n",
    "    esn_pipeline = Pipeline(steps=[(\"har_features\", har_features),\n",
    "                                   (\"scaler\", MinMaxScaler()),\n",
    "                                   (\"esn\", TransformedTargetRegressor(\n",
    "                                       regressor=PyESN(**initial_esn_params),\n",
    "                                       transformer=MinMaxScaler()))\n",
    "                                  ])\n",
    "    # Run model selection\n",
    "    step1_params = {'esn__regressor__input_scaling': uniform(loc=1e-2, scale=1),\n",
    "                    'esn__regressor__spectral_radius': uniform(loc=0, scale=2)}\n",
    "    step2_params = {'esn__regressor__n_reservoir':\n",
    "                        [50, 100, 200, 400, 800, 1600, 3200, 6400],\n",
    "                    'esn__regressor__noise': loguniform(1e-5, 1e1)}\n",
    "    \n",
    "    kwargs_step1 = {'n_iter': 200, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step2 = {'n_iter': 200, 'random_state': 42, 'verbose': 1,\n",
    "                    'n_jobs': -1, 'scoring': scoring, \"refit\": \"R2\",\n",
    "                    \"cv\": ps, \"return_train_score\": True}\n",
    "\n",
    "    searches = [\n",
    "        ('step1', RandomizedSearchCV, step1_params, kwargs_step1),\n",
    "        ('step2', RandomizedSearchCV, step2_params, kwargs_step2)]\n",
    "    search = SequentialSearchCV(esn_pipeline, searches=searches).fit(X, y)\n",
    "    dump(search, f'../results/pyesn_har_seq_esn_h{H}.joblib')\n",
    "print(search.all_best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18658d10-e66c-48f4-a722-a442364e3b37",
   "metadata": {},
   "source": [
    "We do an additional GridSearchCV without parameters for the final evaluation on the test set, because the object returns a mass of useful information.\n",
    "\n",
    "In this step, no parameters are optimized anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c2346b-c3e5-477a-b5c5-8efe680171cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search_test = load(f'../results/pyesn_har_seq_esn_h{H}_test.joblib')\n",
    "except FileNotFoundError:\n",
    "    search_test = GridSearchCV(estimator=clone(search.best_estimator_), param_grid={}, cv=ps_test,\n",
    "                               scoring=scoring, refit=\"R2\", return_train_score=True).fit(X, y)\n",
    "    dump(search_test, f'../results/pyesn_har_seq_esn_h{H}_test.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e9735a-32a1-43d9-be43-e5355df550f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"R^2 train\\tR^2 test\\tMSE train\\tMSE test\\tFit time in ms\\tScore time in ms\")\n",
    "print(f\"{search_test.cv_results_['mean_train_R2']}\\t{search_test.cv_results_['mean_test_R2']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_train_MSE']}\\t{search_test.cv_results_['mean_test_MSE']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_fit_time']*1e3}\\t{search_test.cv_results_['mean_score_time']*1e3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3702fe07-d633-4f14-a849-b8b693900af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(context=\"paper\", style=\"whitegrid\", font=\"serif\", font_scale=.8, rc={\"lines.linewidth\": 0.8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0587d289-0df2-4e3c-bd63-3d4a654b4e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "search1 = load(f\"results/pyesn_har_seq_esn_h{H}.joblib\")\n",
    "df1 = pd.DataFrame(search1.cv_results_).rename(\n",
    "    columns={\"param_esn__regressor__n_reservoir\": \"param_esn__regressor__hidden_layer_size\"})\n",
    "df1[\"Architecture\"] = \"PyESN\"\n",
    "\n",
    "search2 = load(f\"results/pyrcn_har_seq_esn_h{H}.joblib\")\n",
    "df2 = pd.DataFrame(search2.cv_results_)\n",
    "df2[\"Architecture\"] = \"PyRCN\"\n",
    "df = pd.concat([df1, df2]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2452931f-6b4e-4931-9c08-03e802558ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.mean(df1.groupby(by=\"param_esn__regressor__hidden_layer_size\").mean()[\"mean_score_time\"] / df2.groupby(by=\"param_esn__regressor__hidden_layer_size\").mean()[\"mean_score_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249ec57c-ee6b-4abe-8cba-b052ba705019",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.groupby(by=\"param_esn__regressor__hidden_layer_size\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc4ce60-5e15-4ec1-b2f5-fefb1b6acf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.groupby(by=\"param_esn__regressor__hidden_layer_size\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65589b0-2643-45ab-aaea-8b89b0bd9391",
   "metadata": {},
   "source": [
    "TODO: Update the mean speedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a9d82e-f359-4d82-b66f-1fd901dddeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "fig.set_size_inches(4/1.6, 2.5/1.6)\n",
    "sns.pointplot(data=df, x=\"param_esn__regressor__hidden_layer_size\", y=\"mean_fit_time\", hue=\"Architecture\", linestyles=[\":\", \":\"], markers=\"X\", ci=\"sd\", ax=axs)\n",
    "sns.pointplot(data=df, x=\"param_esn__regressor__hidden_layer_size\", y=\"mean_score_time\", hue=\"Architecture\", ci=\"sd\", ax=axs)\n",
    "axs.legend(handles=axs.lines[::8+1], labels=[\"PyESN fit time\", \"PyESN score time\", \"PyRCN fit time\", \"PyRCN score time\"],\n",
    "           loc='upper left', bbox_to_anchor=(0.0, 1.0), ncol=1)\n",
    "axs.set_yscale('log')\n",
    "axs.set(xlabel=\"Reservoir size\", ylabel=\"Time in seconds\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig('ESN_Benchmark.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88fd9e1-33db-498a-b210-7f78896631dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "search1 = load(f\"results/skelm_har_seq_elm_h{H}.joblib\")\n",
    "df1 = pd.DataFrame(search1.cv_results_).rename(\n",
    "    columns={\"param_elm__regressor__n_neurons\": \"param_elm__regressor__hidden_layer_size\"})\n",
    "df1[\"Architecture\"] = \"HP-ELM\"\n",
    "\n",
    "search2 = load(f\"results/pyrcn_har_seq_elm_h{H}.joblib\")\n",
    "df2 = pd.DataFrame(search2.cv_results_)\n",
    "df2[\"Architecture\"] = \"PyRCN\"\n",
    "df = pd.concat([df1, df2]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285a8713-e999-44c2-b3a7-58f67245df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "fig.set_size_inches(4/1.6, 2.5/1.6)\n",
    "sns.pointplot(data=df, x=\"param_elm__regressor__hidden_layer_size\", y=\"mean_fit_time\", hue=\"Architecture\", linestyles=[\":\", \":\"], markers=\"X\", ci=\"sd\", ax=axs)\n",
    "sns.pointplot(data=df, x=\"param_elm__regressor__hidden_layer_size\", y=\"mean_score_time\", hue=\"Architecture\", ci=\"sd\", ax=axs)\n",
    "axs.legend(handles=axs.lines[::8+1], labels=[\"HP-ELM fit time\", \"HP-ELM score time\", \"PyRCN fit time\", \"PyRCN score time\"],\n",
    "           loc='upper left', bbox_to_anchor=(0.0, 1.0), ncol=1)\n",
    "axs.set_yscale('log')\n",
    "axs.set(xlabel=\"Reservoir size\", ylabel=\"Time in seconds\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig('ELM_Benchmark.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a95ec53-5d6e-4056-a276-2c426d47b21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.sort_values(by=\"param_elm__regressor__hidden_layer_size\").reset_index(drop=True)\n",
    "df2 = df2.sort_values(by=\"param_elm__regressor__hidden_layer_size\").reset_index(drop=True)\n",
    "df1[\"fit_speedup\"] = df1[\"mean_fit_time\"] / df2[\"mean_fit_time\"]\n",
    "df1[\"score_speedup\"] = df1[\"mean_score_time\"] / df2[\"mean_score_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e154ef34-9a49-4c09-828e-5421f5e1326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1[\"params\"][-1:], df2[\"params\"][-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48a7279-288f-40e5-baf4-2138da66ad64",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "# fig.set_size_inches(4/1.6, 2.5/1.6)\n",
    "sns.pointplot(data=df1, x=\"param_elm__regressor__hidden_layer_size\", y=\"fit_speedup\", linestyles=\":\", markers=\"X\", ci=\"sd\", ax=axs)\n",
    "sns.pointplot(data=df1, x=\"param_elm__regressor__hidden_layer_size\", y=\"score_speedup\", ci=\"sd\", ax=axs)\n",
    "axs.legend(handles=axs.lines[::8+1], labels=[\"fit time\", \"PyESN score time\", \"PyRCN fit time\", \"PyRCN score time\"],\n",
    "           loc='upper left', bbox_to_anchor=(0.0, 1.0), ncol=1)\n",
    "axs.set_yscale('log')\n",
    "axs.set(xlabel=\"Reservoir size\", ylabel=\"Time in seconds\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig('ESN_speedup.pdf', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a653ea-cf88-4856-8123-1f027bb3772a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.mean(df1[\"fit_speedup\"][:31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523a2adc-2477-4698-b872-6efd6fafd988",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[[\"param_elm__regressor__hidden_layer_size\", \"mean_fit_time\", \"mean_score_time\"]][-31:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3847cebf-1122-4f2b-b348-20b2ca2c9f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[[\"param_elm__regressor__hidden_layer_size\", \"mean_fit_time\", \"mean_score_time\"]][-31:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b6eb60-7dc2-4ea8-b954-566654f5a11b",
   "metadata": {},
   "source": [
    "<a id=\"1\">[1]</a> \n",
    "Gabriel Trierweiler Ribeiro, AndrÃ© Alves Portela Santos, Viviana Cocco Mariani, Leandro dos Santos Coelho. (2021). \n",
    "Novel hybrid model based on echo state neural network applied to the prediction of stock price return volatility. \n",
    "Expert Systems with Applications, 184, 115490. \n",
    "[10.1016/j.eswa.2021.115490](https://doi.org/10.1016/j.eswa.2021.115490)\n",
    "\n",
    "<a id=\"2\">[2]</a> \n",
    "Anton Akusok, Kaj-Mikael BjÃ¶rk, Yoan Miche, Amaury Lendasse. (2015). \n",
    "High-Performance Extreme Learning Machines: A Complete Toolbox for Big Data Applications.\n",
    "Access, IEEE, 3, 1011-1025. \n",
    "[10.1109/ACCESS.2015.2450498](https://doi.org/10.1109/ACCESS.2015.2450498)\n",
    "\n",
    "<a id=\"3\">[3]</a> \n",
    "Fulvio Corsi. (2009). \n",
    "A Simple Approximate Long-Memory Model of Realized Volatility.\n",
    "Journal of Financial Econometrics, 7, 174-196. \n",
    "[10.1093/jjfinec/nbp001](https://doi.org/10.1093/jjfinec/nbp001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5dcd1b-8cdb-4fad-809d-d1b76c80e2a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
