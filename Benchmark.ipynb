{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "033f4152-7dff-4ce3-a0da-26dee5aa3e45",
   "metadata": {},
   "source": [
    "# Benchmark PyRCN\n",
    "\n",
    "This notebook accompanies the publication \"PyRCN: A Toolbox for Exploration and Application of Reservoir Computing Networks\" and serves as a benchmark test for PyRCN and other libraries.\n",
    "\n",
    "Technical analysis -> Chart Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e078dadd-e54c-42e2-a2a4-786534e52b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import MinMaxScaler, FunctionTransformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.utils.fixes import loguniform\n",
    "from scipy.stats import uniform\n",
    "from joblib import dump, load\n",
    "\n",
    "from pyrcn.extreme_learning_machine import ELMRegressor\n",
    "from pyrcn.echo_state_network import ESNRegressor\n",
    "from pyrcn.model_selection import SequentialSearchCV\n",
    "from skelm import ELMRegressor as SkELMRegressor\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.preprocessing import ts2super, compute_average_volatility\n",
    "from src.model_selection import PredefinedTrainValidationTestSplit\n",
    "from src.adapter import PyESN\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab7fa07-7dab-412d-b044-de201098c590",
   "metadata": {},
   "source": [
    "## Prepare all datasets\n",
    "\n",
    "We load the datasets as pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d682b5dc-6632-4184-8dac-0f0fe2f42ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = OrderedDict({\"CAT\": 0, \"EBAY\": 1, \"MSFT\": 2})\n",
    "data = [None] * len(datasets)\n",
    "\n",
    "for dataset, k in datasets.items():\n",
    "    data[k] = pd.read_csv(f\"./data/{dataset}.csv\")[\"t0\"].to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532cb2f1-f95d-41be-8ee8-54df39675d3b",
   "metadata": {},
   "source": [
    "Next, we create feature extraction pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06e741b8-5778-4b3f-b7bb-10b599e4512d",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_volatility_transformer = FunctionTransformer(\n",
    "    func=compute_average_volatility, kw_args={\"window_length\": 1})\n",
    "week_volatility_transformer = FunctionTransformer(\n",
    "    func=compute_average_volatility, kw_args={\"window_length\": 5})\n",
    "month_volatility_transformer = FunctionTransformer(\n",
    "    func=compute_average_volatility, kw_args={\"window_length\": 22})\n",
    "har_features = FeatureUnion(\n",
    "    transformer_list=[(\"day\", day_volatility_transformer),\n",
    "                      (\"week\", week_volatility_transformer),\n",
    "                      (\"month\", month_volatility_transformer)])\n",
    "har_pipeline = Pipeline(\n",
    "    steps=[(\"har_features\", har_features),\n",
    "           (\"scaler\", MinMaxScaler()),\n",
    "           (\"lstsq\", TransformedTargetRegressor(transformer=MinMaxScaler()))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86906f83-ba62-4132-8a91-de2a933fb718",
   "metadata": {},
   "source": [
    "We define a forecasting horizon for which we want to optimize and analyze models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f0f1b8ce-184e-49e5-bbc9-e45c0c47b40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "H=21  # pre-trained models available for 1, 5, 21\n",
    "\n",
    "# ts2super simply returns a dataframe containing the original and shifted by H time series.\n",
    "df = pd.concat([ts2super(d, 0, H) for d in data])\n",
    "X = df.iloc[:, 0].values.reshape(-1, 1)\n",
    "y = df.iloc[:, -1].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "10d95bb9-633f-49c1-be23-4dd606cde60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t0</th>\n",
       "      <th>t5</th>\n",
       "      <th>t22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000832</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>0.000314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2719</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2720</th>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2721</th>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2722</th>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2723</th>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8172 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            t0        t5       t22\n",
       "0     0.000223  0.000223  0.000223\n",
       "1     0.000121  0.000172  0.000172\n",
       "2     0.000124  0.000156  0.000156\n",
       "3     0.000268  0.000184  0.000184\n",
       "4     0.000832  0.000314  0.000314\n",
       "...        ...       ...       ...\n",
       "2719  0.000048  0.000080  0.000162\n",
       "2720  0.000027  0.000068  0.000150\n",
       "2721  0.000037  0.000057  0.000138\n",
       "2722  0.000070  0.000054  0.000129\n",
       "2723  0.000077  0.000052  0.000105\n",
       "\n",
       "[8172 rows x 3 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_input = df.iloc[:, 0].to_frame()\n",
    "df_input[\"t5\"] = df_input.t0.rolling(window=5, min_periods=1).mean()\n",
    "df_input[\"t22\"] = df_input.t0.rolling(window=22, min_periods=1).mean()\n",
    "df_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9b53c906-7f4a-4fcc-965a-5a22a0d8af84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input_extended = pd.concat([pd.DataFrame(data={\"t0\": [0], \"t5\": [0], \"t22\": [0]}), df_input], ignore_index=False).reset_index(drop=True)\n",
    "df_input_extended[\"t0\"] = df_input_extended[\"t0\"].shift(periods=-1)\n",
    "df_input_extended = df_input_extended.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dedd81c3-0037-43be-975e-3172998d8a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8172 entries, 0 to 8171\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   t0      8172 non-null   float64\n",
      " 1   t5      8172 non-null   float64\n",
      " 2   t22     8172 non-null   float64\n",
      "dtypes: float64(3)\n",
      "memory usage: 255.4 KB\n"
     ]
    }
   ],
   "source": [
    "df_input_extended.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8de015-8681-406f-acaa-f367185dc536",
   "metadata": {},
   "source": [
    "Provide a custom train test split for this task, and all scores that we aim to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c99e7969-b5f9-429d-910f-3c10026413e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fold = [[k] * len(ts2super(d, 0, H)) for k, d in enumerate(data)]\n",
    "test_fold = list(itertools.chain.from_iterable(test_fold))\n",
    "\n",
    "ps = PredefinedTrainValidationTestSplit(test_fold=test_fold, validation=True)\n",
    "ps_test = PredefinedTrainValidationTestSplit(test_fold=test_fold, validation=False)\n",
    "\n",
    "scoring = {\"MSE\": \"neg_mean_squared_error\", \"RMSE\": \"neg_root_mean_squared_error\", \"R2\": \"r2\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532455c7-0d67-4df7-9565-697a66b9aa06",
   "metadata": {},
   "source": [
    "## HAR\n",
    "\n",
    "The HAR (Heterogeneous AutoRegressive) features are, as the name says, autoregressive features. Here, temporal context is considered by smoothing a time series with moving averages of the last week (5 days) and month (22 days). The smoothed variants are finally added to the original time series. Thus, HAR has three features.\n",
    "\n",
    "TODO: HAR-Feature extraction outside of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2fdc806b-3a1f-4afe-abb5-58390d9ae9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "har_pipeline = Pipeline(\n",
    "    steps=[# (\"har_features\", har_features),\n",
    "           (\"scaler\", MinMaxScaler()),\n",
    "           (\"lstsq\", TransformedTargetRegressor(transformer=MinMaxScaler()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "84a8f2c3-1288-474a-bb98-4a447ee27e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV 1/3; 1/1] START ............................................................\n",
      "[CV 1/3; 1/1] END  MSE: (train=-0.000, test=-0.000) R2: (train=0.292, test=0.305) RMSE: (train=-0.000, test=-0.000) total time=   0.0s\n",
      "[CV 2/3; 1/1] START ............................................................\n",
      "[CV 2/3; 1/1] END  MSE: (train=-0.000, test=-0.000) R2: (train=0.311, test=0.241) RMSE: (train=-0.000, test=-0.000) total time=   0.0s\n",
      "[CV 3/3; 1/1] START ............................................................\n",
      "[CV 3/3; 1/1] END  MSE: (train=-0.000, test=-0.000) R2: (train=0.290, test=0.243) RMSE: (train=-0.000, test=-0.000) total time=   0.0s\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    search = load(f'./results/har_grid_h{H}_modified.joblib')\n",
    "except FileNotFoundError:\n",
    "    search = GridSearchCV(estimator=har_pipeline, param_grid={}, cv=ps_test,\n",
    "                          scoring=scoring, refit=\"R2\", return_train_score=True, verbose=10).fit(df_input_extended, y)\n",
    "    dump(search, f'./results/har_grid_h{H}_modified.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0dd8a0e4-bfb6-4801-8c3f-892eed00f9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 train\tR^2 test\tMSE train\tMSE test\tFit time in ms\tScore time in ms\n",
      "[0.29726028]\t[0.2628909]\t[-1.17106478e-07]\t[-1.18731243e-07]\t[13.36868604]\t[5.51358859]\n"
     ]
    }
   ],
   "source": [
    "print(f\"R^2 train\\tR^2 test\\tMSE train\\tMSE test\\tFit time in ms\\tScore time in ms\")\n",
    "print(f\"{search.cv_results_['mean_train_R2']}\\t{search.cv_results_['mean_test_R2']}\\t\"\n",
    "      f\"{search.cv_results_['mean_train_MSE']}\\t{search.cv_results_['mean_test_MSE']}\\t\"\n",
    "      f\"{search.cv_results_['mean_fit_time']*1e3}\\t{search.cv_results_['mean_score_time']*1e3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e6d329-f1fc-426e-b1c3-c2ab73e76cde",
   "metadata": {},
   "source": [
    "## PyRCN ESN\n",
    "\n",
    "Next, we optimize ESNs from PyRCN using only the original time series as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2da7966a-bd89-4135-b0f2-eb39222bef7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'step1': {'esn__regressor__input_scaling': 0.20967378215835974, 'esn__regressor__spectral_radius': 1.0284688768272232}, 'step2': {'esn__regressor__leakage': 0.31172107608941096}, 'step3': {'esn__regressor__bias_scaling': 0.06175348288740734}, 'step4': {'esn__regressor__alpha': 0.0039054412752107916, 'esn__regressor__hidden_layer_size': 50}}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    search = load(f'./results/pyrcn_seq_esn_h{H}.joblib')\n",
    "except FileNotFoundError:\n",
    "    initial_esn_params = {\n",
    "        'hidden_layer_size': 50, 'k_in': 1, 'input_scaling': 0.4, 'input_activation': 'identity',\n",
    "        'bias_scaling': 0.0, 'spectral_radius': 0.0, 'leakage': 1.0, 'k_rec': 10,\n",
    "        'reservoir_activation': 'tanh', 'bidirectional': False, 'alpha': 1e-5, 'random_state': 42}\n",
    "    esn_pipeline = Pipeline(steps=[(\"scaler\", MinMaxScaler()),\n",
    "                                   (\"esn\", TransformedTargetRegressor(\n",
    "                                       regressor=ESNRegressor(**initial_esn_params),\n",
    "                                       transformer=MinMaxScaler()))\n",
    "                                  ])\n",
    "    # Run model selection\n",
    "    step1_params = {'esn__regressor__input_scaling': uniform(loc=1e-2, scale=1),\n",
    "                    'esn__regressor__spectral_radius': uniform(loc=0, scale=2)}\n",
    "    step2_params = {'esn__regressor__leakage': uniform(1e-5, 1e0)}\n",
    "    step3_params = {'esn__regressor__bias_scaling': uniform(loc=0, scale=3)}\n",
    "    step4_params = {'esn__regressor__hidden_layer_size': [50, 100, 200, 400, 800, 1600, 3200, 6400],\n",
    "                    'esn__regressor__alpha': loguniform(1e-5, 1e1)}\n",
    "    \n",
    "    kwargs_step1 = {'n_iter': 200, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': scoring,\n",
    "                    \"refit\": \"R2\", \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step2 = {'n_iter': 50, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': scoring,\n",
    "                    \"refit\": \"R2\", \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step3 = {'n_iter': 50, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': scoring,\n",
    "                    \"refit\": \"R2\", \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step4 = {'n_iter': 200, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': scoring,\n",
    "                    \"refit\": \"R2\", \"cv\": ps, \"return_train_score\": True}\n",
    "\n",
    "    searches = [\n",
    "        ('step1', RandomizedSearchCV, step1_params, kwargs_step1),\n",
    "        ('step2', RandomizedSearchCV, step2_params, kwargs_step2),\n",
    "        ('step3', RandomizedSearchCV, step3_params, kwargs_step3),\n",
    "        ('step4', RandomizedSearchCV, step4_params, kwargs_step4)]\n",
    "    search = SequentialSearchCV(esn_pipeline, searches=searches).fit(X, y)\n",
    "    dump(search, f'./results/pyrcn_seq_esn_h{H}.joblib')\n",
    "print(search.all_best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b4d1c7-d214-4699-ad2c-2766f0c850f7",
   "metadata": {},
   "source": [
    "We do an additional GridSearchCV without parameters, because the object returns a mass of useful information.\n",
    "\n",
    "If we do it without parameters, simply the estimator will be fit on the train test splits defined in our cv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "93f5e590-431a-4b17-8729-2b979b09af19",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search_test = load(f'./results/pyrcn_seq_esn_h{H}_test.joblib')\n",
    "except FileNotFoundError:\n",
    "    search_test = GridSearchCV(estimator=clone(search.best_estimator_), param_grid={}, cv=ps_test,\n",
    "                               scoring=scoring, refit=\"R2\", return_train_score=True).fit(X, y)\n",
    "    dump(search_test, f'./results/pyrcn_seq_esn_h{H}_test.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5d4a1be4-7a80-4d7a-a365-c8252cb2b4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 train\tR^2 test\tMSE train\tMSE test\tFit time in ms\tScore time in ms\n",
      "[0.36171414]\t[0.31596711]\t[-1.06168016e-07]\t[-1.09820392e-07]\t[403.71926626]\t[150.03554026]\n"
     ]
    }
   ],
   "source": [
    "print(f\"R^2 train\\tR^2 test\\tMSE train\\tMSE test\\tFit time in ms\\tScore time in ms\")\n",
    "print(f\"{search_test.cv_results_['mean_train_R2']}\\t{search_test.cv_results_['mean_test_R2']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_train_MSE']}\\t{search_test.cv_results_['mean_test_MSE']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_fit_time']*1e3}\\t{search_test.cv_results_['mean_score_time']*1e3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fcd07f-411c-4022-be8a-6081cb9ab3f6",
   "metadata": {},
   "source": [
    "Next, we optimize ESNs from PyRCN using HAR as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "40c42717-100d-4e5b-bae3-59334ac67a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'step1': {'esn__regressor__input_scaling': 0.1548948720912231, 'esn__regressor__spectral_radius': 0.978905520555126}, 'step2': {'esn__regressor__leakage': 0.30425224295953773}, 'step3': {'esn__regressor__bias_scaling': 0.46798356100860794}, 'step4': {'esn__regressor__alpha': 0.0039054412752107916, 'esn__regressor__hidden_layer_size': 50}}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    search = load(f'./results/pyrcn_har_seq_esn_h{H}.joblib')\n",
    "except FileNotFoundError:\n",
    "    initial_esn_params = {\n",
    "        'hidden_layer_size': 50, 'k_in': 1, 'input_scaling': 0.4, 'input_activation': 'identity',\n",
    "        'bias_scaling': 0.0, 'spectral_radius': 0.0, 'leakage': 1.0, 'k_rec': 10,\n",
    "        'reservoir_activation': 'tanh', 'bidirectional': False, 'alpha': 1e-5, 'random_state': 42}\n",
    "    esn_pipeline = Pipeline(steps=[(\"har_features\", har_features),\n",
    "                                   (\"scaler\", MinMaxScaler()),\n",
    "                                   (\"esn\", TransformedTargetRegressor(\n",
    "                                       regressor=ESNRegressor(**initial_esn_params),\n",
    "                                       transformer=MinMaxScaler()))\n",
    "                                  ])\n",
    "    # Run model selection\n",
    "    step1_params = {'esn__regressor__input_scaling': uniform(loc=1e-2, scale=1),\n",
    "                    'esn__regressor__spectral_radius': uniform(loc=0, scale=2)}\n",
    "    step2_params = {'esn__regressor__leakage': uniform(1e-5, 1e0)}\n",
    "    step3_params = {'esn__regressor__bias_scaling': uniform(loc=0, scale=3)}\n",
    "    step4_params = {'esn__regressor__hidden_layer_size': [50, 100, 200, 400, 800, 1600, 3200, 6400],\n",
    "                    'esn__regressor__alpha': loguniform(1e-5, 1e1)}\n",
    "    \n",
    "    kwargs_step1 = {'n_iter': 200, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': scoring,\n",
    "                    \"refit\": \"R2\", \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step2 = {'n_iter': 50, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': scoring,\n",
    "                    \"refit\": \"R2\", \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step3 = {'n_iter': 50, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': scoring,\n",
    "                    \"refit\": \"R2\", \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step4 = {'n_iter': 200, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': scoring,\n",
    "                    \"refit\": \"R2\", \"cv\": ps, \"return_train_score\": True}\n",
    "\n",
    "    searches = [\n",
    "        ('step1', RandomizedSearchCV, step1_params, kwargs_step1),\n",
    "        ('step2', RandomizedSearchCV, step2_params, kwargs_step2),\n",
    "        ('step3', RandomizedSearchCV, step3_params, kwargs_step3),\n",
    "        ('step4', RandomizedSearchCV, step4_params, kwargs_step4)]\n",
    "    search = SequentialSearchCV(esn_pipeline, searches=searches).fit(X, y)\n",
    "    dump(search, f'./results/pyrcn_har_seq_esn_h{H}.joblib')\n",
    "print(search.all_best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a10d2e-9b39-4dcd-9e1c-aec94209af8c",
   "metadata": {},
   "source": [
    "We do an additional GridSearchCV without parameters, because the object returns a mass of useful information.\n",
    "\n",
    "If we do it without parameters, simply the estimator will be fit on the train test splits defined in our cv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "59fcf3f0-ba5f-4b38-b44e-59709fc123f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search_test = load(f'./results/pyrcn_har_seq_esn_h{H}_test.joblib')\n",
    "except FileNotFoundError:\n",
    "    search_test = GridSearchCV(estimator=clone(search.best_estimator_), param_grid={}, cv=ps_test,\n",
    "                               scoring=scoring, refit=\"R2\", return_train_score=True).fit(X, y)\n",
    "    dump(search_test, f'./results/pyrcn_har_seq_esn_h{H}_test.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f83ec0d2-c450-44aa-86ee-e473d0520e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 train\tR^2 test\tMSE train\tMSE test\tFit time in ms\tScore time in ms\n",
      "[0.35745346]\t[0.30969009]\t[-1.0706213e-07]\t[-1.11069823e-07]\t[416.5306886]\t[174.58772659]\n"
     ]
    }
   ],
   "source": [
    "print(f\"R^2 train\\tR^2 test\\tMSE train\\tMSE test\\tFit time in ms\\tScore time in ms\")\n",
    "print(f\"{search_test.cv_results_['mean_train_R2']}\\t{search_test.cv_results_['mean_test_R2']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_train_MSE']}\\t{search_test.cv_results_['mean_test_MSE']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_fit_time']*1e3}\\t{search_test.cv_results_['mean_score_time']*1e3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c42c68-5e1a-455e-94a8-d16dd375c6fa",
   "metadata": {},
   "source": [
    "## PyRCN ELM\n",
    "\n",
    "Next, we optimize ELMs from PyRCN using only the original time series as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "475b9669-9977-4030-ace0-868442cf2386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'step1': {'elm__regressor__input_scaling': 0.7419939418114051}, 'step2': {'elm__regressor__bias_scaling': 0.06175348288740734}, 'step3': {'elm__regressor__alpha': 0.0039054412752107916, 'elm__regressor__hidden_layer_size': 50}}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    search = load(f'./results/pyrcn_seq_elm_h{H}.joblib')\n",
    "except FileNotFoundError:\n",
    "    initial_elm_params = {\n",
    "        'hidden_layer_size': 50, 'k_in': 1, 'input_scaling': 0.4, 'input_activation': 'tanh',\n",
    "        'bias_scaling': 0.0, 'alpha': 1e-5, 'random_state': 42}\n",
    "    elm_pipeline = Pipeline(steps=[(\"scaler\", MinMaxScaler()),\n",
    "                                   (\"'random_state': 42\", TransformedTargetRegressor(\n",
    "                                       regressor=ELMRegressor(**initial_esn_params),\n",
    "                                       transformer=MinMaxScaler()))\n",
    "                                  ])\n",
    "    # Run model selection\n",
    "    step1_params = {'elm__regressor__input_scaling': uniform(loc=1e-2, scale=1)}\n",
    "    step2_params = {'esn__regressor__bias_scaling': uniform(loc=0, scale=3)}\n",
    "    step3_params = {'elm__regressor__hidden_layer_size': [50, 100, 200, 400, 800, 1600, 3200, 6400],\n",
    "                    'elm__regressor__alpha': loguniform(1e-5, 1e1)}\n",
    "    \n",
    "    kwargs_step1 = {'n_iter': 50, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': scoring,\n",
    "                    \"refit\": \"R2\", \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step2 = {'n_iter': 50, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': scoring,\n",
    "                    \"refit\": \"R2\", \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step3 = {'n_iter': 200, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': scoring,\n",
    "                    \"refit\": \"R2\", \"cv\": ps, \"return_train_score\": True}\n",
    "\n",
    "    searches = [\n",
    "        ('step1', RandomizedSearchCV, step1_params, kwargs_step1),\n",
    "        ('step2', RandomizedSearchCV, step2_params, kwargs_step2),\n",
    "        ('step3', RandomizedSearchCV, step3_params, kwargs_step3)]\n",
    "    search = SequentialSearchCV(elm_pipeline, searches=searches).fit(X, y)\n",
    "    dump(search, f'./results/pyrcn_seq_elm_h{H}.joblib')\n",
    "print(search.all_best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e0f5d6-738d-4cbe-8328-c0d1a1c04af2",
   "metadata": {},
   "source": [
    "We do an additional GridSearchCV without parameters, because the object returns a mass of useful information.\n",
    "\n",
    "If we do it without parameters, simply the estimator will be fit on the train test splits defined in our cv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d51c5c19-14e5-4d42-880a-0e48b062e496",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search_test = load(f'./results/pyrcn_seq_elm_h{H}_test.joblib')\n",
    "except FileNotFoundError:\n",
    "    search_test = GridSearchCV(estimator=clone(search.best_estimator_), param_grid={}, cv=ps_test,\n",
    "                               scoring=scoring, refit=\"R2\", return_train_score=True).fit(X, y)\n",
    "    dump(search_test, f'./results/pyrcn_seq_elm_h{H}_test.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "da8c90d1-c9c9-4ee3-a356-2cd003e271a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 train\tR^2 test\tMSE train\tMSE test\tFit time in ms\tScore time in ms\n",
      "[0.25572784]\t[0.21816533]\t[-1.23882321e-07]\t[-1.26022426e-07]\t[33.09901555]\t[10.55185]\n"
     ]
    }
   ],
   "source": [
    "print(f\"R^2 train\\tR^2 test\\tMSE train\\tMSE test\\tFit time in ms\\tScore time in ms\")\n",
    "print(f\"{search_test.cv_results_['mean_train_R2']}\\t{search_test.cv_results_['mean_test_R2']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_train_MSE']}\\t{search_test.cv_results_['mean_test_MSE']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_fit_time']*1e3}\\t{search_test.cv_results_['mean_score_time']*1e3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e59891-409d-4979-bab2-6a017fb0ff9f",
   "metadata": {},
   "source": [
    "Next, we optimize ELMs from PyRCN using HAR as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ac485279-fccf-47e8-9bf7-1570101fd29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'step1': {'elm__regressor__input_scaling': 0.10767211400638386}, 'step2': {'elm__regressor__bias_scaling': 0.06175348288740734}, 'step3': {'elm__regressor__alpha': 0.0039054412752107916, 'elm__regressor__hidden_layer_size': 50}}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    search = load(f'./results/pyrcn_har_seq_elm_h{H}.joblib')\n",
    "except FileNotFoundError:\n",
    "    initial_elm_params = {\n",
    "        'hidden_layer_size': 50, 'k_in': 1, 'input_scaling': 0.4, 'input_activation': 'tanh',\n",
    "        'bias_scaling': 0.0, 'alpha': 1e-5, 'random_state': 42}\n",
    "    elm_pipeline = Pipeline(steps=[(\"har_features\", har_features),\n",
    "                                   (\"scaler\", MinMaxScaler()),\n",
    "                                   (\"'random_state': 42\", TransformedTargetRegressor(\n",
    "                                       regressor=ELMRegressor(**initial_esn_params),\n",
    "                                       transformer=MinMaxScaler()))\n",
    "                                  ])\n",
    "    # Run model selection\n",
    "    step1_params = {'elm__regressor__input_scaling': uniform(loc=1e-2, scale=1)}\n",
    "    step2_params = {'elm__regressor__bias_scaling': uniform(loc=0, scale=3)}\n",
    "    step3_params = {'elm__regressor__hidden_layer_size': [50, 100, 200, 400, 800, 1600, 3200, 6400],\n",
    "                    'elm__regressor__alpha': loguniform(1e-5, 1e1)}\n",
    "    \n",
    "    kwargs_step1 = {'n_iter': 50, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': scoring,\n",
    "                    \"refit\": \"R2\", \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step2 = {'n_iter': 50, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': scoring,\n",
    "                    \"refit\": \"R2\", \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step3 = {'n_iter': 200, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': scoring,\n",
    "                    \"refit\": \"R2\", \"cv\": ps, \"return_train_score\": True}\n",
    "\n",
    "    searches = [\n",
    "        ('step1', RandomizedSearchCV, step1_params, kwargs_step1),\n",
    "        ('step2', RandomizedSearchCV, step2_params, kwargs_step2),\n",
    "        ('step3', RandomizedSearchCV, step3_params, kwargs_step3)]\n",
    "    search = SequentialSearchCV(elm_pipeline, searches=searches).fit(X, y)\n",
    "    dump(search, f'./results/pyrcn_har_seq_elm_h{H}.joblib')\n",
    "print(search.all_best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa97d242-c8a2-429a-b4ea-b78a77b5cf78",
   "metadata": {},
   "source": [
    "We do an additional GridSearchCV without parameters, because the object returns a mass of useful information.\n",
    "\n",
    "If we do it without parameters, simply the estimator will be fit on the train test splits defined in our cv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e9d63bda-06ac-4155-b097-50ddf1a70488",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search_test = load(f'./results/pyrcn_har_seq_elm_h{H}_test.joblib')\n",
    "except FileNotFoundError:\n",
    "    search_test = GridSearchCV(estimator=clone(search.best_estimator_), param_grid={}, cv=ps_test,\n",
    "                               scoring=scoring, refit=\"R2\", return_train_score=True).fit(X, y)\n",
    "    dump(search_test, f'./results/pyrcn_har_seq_elm_h{H}_test.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "40c6b0a2-1660-4361-a9e5-2121108dda94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 train\tR^2 test\tMSE train\tMSE test\tFit time in ms\tScore time in ms\n",
      "[0.3056951]\t[0.27217819]\t[-1.15662543e-07]\t[-1.1714314e-07]\t[31.37811025]\t[10.69545746]\n"
     ]
    }
   ],
   "source": [
    "print(f\"R^2 train\\tR^2 test\\tMSE train\\tMSE test\\tFit time in ms\\tScore time in ms\")\n",
    "print(f\"{search_test.cv_results_['mean_train_R2']}\\t{search_test.cv_results_['mean_test_R2']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_train_MSE']}\\t{search_test.cv_results_['mean_test_MSE']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_fit_time']*1e3}\\t{search_test.cv_results_['mean_score_time']*1e3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160dc009-3ec4-4eba-8ca6-3cc64b4430bc",
   "metadata": {},
   "source": [
    "## HP-ELM\n",
    "\n",
    "Next, we optimize ELMs from HP-ELM using only the original time series as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f3eb0b5a-a034-4d69-be5e-3bc41efab22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'step1': {'elm__regressor__alpha': 1.3992515562068698e-05, 'elm__regressor__n_neurons': 50}}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    search = load(f'./results/skelm_seq_elm_h{H}.joblib')\n",
    "except FileNotFoundError:\n",
    "    initial_elm_params = {\n",
    "        'hidden_layer_size': 50, 'k_in': 1, 'input_scaling': 0.4, 'input_activation': 'tanh',\n",
    "        'bias_scaling': 0.0, 'alpha': 1e-5, 'random_state': 42}\n",
    "    elm_pipeline = Pipeline(steps=[(\"scaler\", MinMaxScaler()),\n",
    "                                   (\"'random_state': 42\", TransformedTargetRegressor(\n",
    "                                       regressor=ELMRegressor(**initial_esn_params),\n",
    "                                       transformer=MinMaxScaler()))\n",
    "                                  ])\n",
    "    # Run model selection\n",
    "    step1_params = {'elm__regressor__input_scaling': uniform(loc=1e-2, scale=1)}\n",
    "    step2_params = {'elm__regressor__bias_scaling': uniform(loc=0, scale=3)}\n",
    "    step3_params = {'elm__regressor__hidden_layer_size': [50, 100, 200, 400, 800, 1600, 3200, 6400],\n",
    "                    'elm__regressor__alpha': loguniform(1e-5, 1e1)}\n",
    "    \n",
    "    kwargs_step1 = {'n_iter': 50, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': scoring,\n",
    "                    \"refit\": \"R2\", \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step2 = {'n_iter': 50, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': scoring,\n",
    "                    \"refit\": \"R2\", \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step3 = {'n_iter': 200, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': scoring,\n",
    "                    \"refit\": \"R2\", \"cv\": ps, \"return_train_score\": True}\n",
    "\n",
    "    searches = [\n",
    "        ('step1', RandomizedSearchCV, step1_params, kwargs_step1),\n",
    "        ('step2', RandomizedSearchCV, step2_params, kwargs_step2),\n",
    "        ('step3', RandomizedSearchCV, step3_params, kwargs_step3)]\n",
    "    search = SequentialSearchCV(elm_pipeline, searches=searches).fit(X, y)\n",
    "    dump(search, f'./results/skelm_seq_elm_h{H}.joblib')\n",
    "print(search.all_best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd523fb1-d360-4b72-a640-07cacc4831da",
   "metadata": {},
   "source": [
    "We do an additional GridSearchCV without parameters, because the object returns a mass of useful information.\n",
    "\n",
    "If we do it without parameters, simply the estimator will be fit on the train test splits defined in our cv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d0a60403-de08-4d60-9b48-971ecbbacef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search_test = load(f'./results/skelm_seq_elm_h{H}_test.joblib')\n",
    "except FileNotFoundError:\n",
    "    search_test = GridSearchCV(estimator=clone(search.best_estimator_), param_grid={}, cv=ps_test,\n",
    "                               scoring=scoring, refit=\"R2\", return_train_score=True).fit(X, y)\n",
    "    dump(search_test, f'./results/skelm_seq_elm_h{H}_test.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e8f62125-6690-42f6-9668-055ffe3d6a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 train\tR^2 test\tMSE train\tMSE test\tFit time in ms\tScore time in ms\n",
      "[0.23639608]\t[0.19482808]\t[-1.27086115e-07]\t[-1.29155482e-07]\t[18.56168111]\t[8.52592786]\n"
     ]
    }
   ],
   "source": [
    "print(f\"R^2 train\\tR^2 test\\tMSE train\\tMSE test\\tFit time in ms\\tScore time in ms\")\n",
    "print(f\"{search_test.cv_results_['mean_train_R2']}\\t{search_test.cv_results_['mean_test_R2']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_train_MSE']}\\t{search_test.cv_results_['mean_test_MSE']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_fit_time']*1e3}\\t{search_test.cv_results_['mean_score_time']*1e3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f045a5e3-ed10-4f0c-b2bc-e21edab91e4c",
   "metadata": {},
   "source": [
    "Next, we optimize ELMs from HP-ELM using HAR as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f5a14f20-03fd-4e90-a31f-467cbba3ac47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'step1': {'elm__regressor__alpha': 1.453542417265149, 'elm__regressor__n_neurons': 50}}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    search = load(f'./results/skelm_har_seq_elm_h{H}.joblib')\n",
    "except FileNotFoundError:\n",
    "    initial_elm_params = {\n",
    "        'hidden_layer_size': 50, 'k_in': 1, 'input_scaling': 0.4, 'input_activation': 'tanh',\n",
    "        'bias_scaling': 0.0, 'alpha': 1e-5, 'random_state': 42}\n",
    "    elm_pipeline = Pipeline(steps=[(\"har_features\", har_features),\n",
    "                                   (\"scaler\", MinMaxScaler()),\n",
    "                                   (\"'random_state': 42\", TransformedTargetRegressor(\n",
    "                                       regressor=ELMRegressor(**initial_esn_params),\n",
    "                                       transformer=MinMaxScaler()))\n",
    "                                  ])\n",
    "    # Run model selection\n",
    "    step1_params = {'elm__regressor__input_scaling': uniform(loc=1e-2, scale=1)}\n",
    "    step2_params = {'elm__regressor__bias_scaling': uniform(loc=0, scale=3)}\n",
    "    step3_params = {'elm__regressor__hidden_layer_size': [50, 100, 200, 400, 800, 1600, 3200, 6400],\n",
    "                    'elm__regressor__alpha': loguniform(1e-5, 1e1)}\n",
    "    \n",
    "    kwargs_step1 = {'n_iter': 50, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': scoring,\n",
    "                    \"refit\": \"R2\", \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step2 = {'n_iter': 50, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': scoring,\n",
    "                    \"refit\": \"R2\", \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step3 = {'n_iter': 200, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': scoring,\n",
    "                    \"refit\": \"R2\", \"cv\": ps, \"return_train_score\": True}\n",
    "\n",
    "    searches = [\n",
    "        ('step1', RandomizedSearchCV, step1_params, kwargs_step1),\n",
    "        ('step2', RandomizedSearchCV, step2_params, kwargs_step2),\n",
    "        ('step3', RandomizedSearchCV, step3_params, kwargs_step3)]\n",
    "    search = SequentialSearchCV(esn_pipeline, searches=searches).fit(X, y)\n",
    "    dump(search, f'./results/skelm_har_seq_elm_h{H}.joblib')\n",
    "print(search.all_best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e55b881-821b-476f-9a13-3740dbd9bc51",
   "metadata": {},
   "source": [
    "We do an additional GridSearchCV without parameters, because the object returns a mass of useful information.\n",
    "\n",
    "If we do it without parameters, simply the estimator will be fit on the train test splits defined in our cv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6b5305b0-77e0-4371-906f-7add1e3d6ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search_test = load(f'./results/pyrcn_har_seq_elm_h{H}_test.joblib')\n",
    "except FileNotFoundError:\n",
    "    search_test = GridSearchCV(estimator=clone(search.best_estimator_), param_grid={}, cv=ps_test,\n",
    "                               scoring=scoring, refit=\"R2\", return_train_score=True).fit(X, y)\n",
    "    dump(search_test, f'./results/pyrcn_har_seq_elm_h{H}_test.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "858af3b1-a9c5-4598-8404-39c67036b7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 train\tR^2 test\tMSE train\tMSE test\tFit time in ms\tScore time in ms\n",
      "[0.3056951]\t[0.27217819]\t[-1.15662543e-07]\t[-1.1714314e-07]\t[31.37811025]\t[10.69545746]\n"
     ]
    }
   ],
   "source": [
    "print(f\"R^2 train\\tR^2 test\\tMSE train\\tMSE test\\tFit time in ms\\tScore time in ms\")\n",
    "print(f\"{search_test.cv_results_['mean_train_R2']}\\t{search_test.cv_results_['mean_test_R2']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_train_MSE']}\\t{search_test.cv_results_['mean_test_MSE']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_fit_time']*1e3}\\t{search_test.cv_results_['mean_score_time']*1e3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474a7e30-3e2f-4afe-acb9-7226fc130324",
   "metadata": {},
   "source": [
    "## PyESN ESN\n",
    "\n",
    "Next, we optimize ESNs from PyESN using only the original time series as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cf1b22cb-39cd-483a-bb32-9ea4f44f5347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'step1': {'esn__regressor__input_scaling': 0.1295942459383017, 'esn__regressor__spectral_radius': 1.42648957444599}, 'step2': {'esn__regressor__n_reservoir': 50, 'esn__regressor__noise': 1.246802066209201e-05}}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    search = load(f'./results/pyesn_seq_esn_h{H}.joblib')\n",
    "except FileNotFoundError:\n",
    "    initial_esn_params = {\n",
    "        'n_inputs': 1, 'n_outputs': 1, 'n_reservoir': 50, 'spectral_radius': 0.0, 'sparsity': 0.5,\n",
    "        'noise': 1e-3, 'input_shift': 0, 'input_scaling': 0.4, 'teacher_forcing': True,\n",
    "        'teacher_scaling': 1.,'teacher_shift': 0, 'random_state': 42}\n",
    "    esn_pipeline = Pipeline(steps=[(\"scaler\", MinMaxScaler()),\n",
    "                                   (\"esn\", TransformedTargetRegressor(\n",
    "                                       regressor=PyESN(**initial_esn_params),\n",
    "                                       transformer=MinMaxScaler()))\n",
    "                                  ])\n",
    "    # Run model selection\n",
    "    step1_params = {'esn__regressor__input_scaling': uniform(loc=1e-2, scale=1),\n",
    "                    'esn__regressor__spectral_radius': uniform(loc=0, scale=2)}\n",
    "    step2_params = {'esn__regressor__n_reservoir': [50, 100, 200, 400, 800, 1600, 3200, 6400],\n",
    "                    'esn__regressor__noise': loguniform(1e-5, 1e1)}\n",
    "    \n",
    "    kwargs_step1 = {'n_iter': 200, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': scoring,\n",
    "                    \"refit\": \"R2\", \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step2 = {'n_iter': 200, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': scoring,\n",
    "                    \"refit\": \"R2\", \"cv\": ps, \"return_train_score\": True}\n",
    "\n",
    "    searches = [\n",
    "        ('step1', RandomizedSearchCV, step1_params, kwargs_step1),\n",
    "        ('step2', RandomizedSearchCV, step2_params, kwargs_step2)]\n",
    "    search = SequentialSearchCV(esn_pipeline, searches=searches).fit(X, y)\n",
    "    dump(search, f'./results/pyesn_seq_esn_h{H}.joblib')\n",
    "print(search.all_best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d514d1-8686-46db-bfaf-9ad21c4682d4",
   "metadata": {},
   "source": [
    "We do an additional GridSearchCV without parameters, because the object returns a mass of useful information.\n",
    "\n",
    "If we do it without parameters, simply the estimator will be fit on the train test splits defined in our cv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6a2044c0-f435-4e1f-8658-49422d96ebca",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search_test = load(f'./results/pyesn_seq_esn_h{H}_test.joblib')\n",
    "except FileNotFoundError:\n",
    "    search_test = GridSearchCV(estimator=clone(search.best_estimator_), param_grid={}, cv=ps_test,\n",
    "                               scoring=scoring, refit=\"R2\", return_train_score=True).fit(X, y)\n",
    "    dump(search_test, f'./results/pyesn_seq_esn_h{H}_test.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "378af756-3f32-4133-9121-8ec7e17a7ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 train\tR^2 test\tMSE train\tMSE test\tFit time in ms\tScore time in ms\n",
      "[0.28610938]\t[0.26937921]\t[-1.19052607e-07]\t[-1.17368253e-07]\t[107.71107674]\t[53.97764842]\n"
     ]
    }
   ],
   "source": [
    "print(f\"R^2 train\\tR^2 test\\tMSE train\\tMSE test\\tFit time in ms\\tScore time in ms\")\n",
    "print(f\"{search_test.cv_results_['mean_train_R2']}\\t{search_test.cv_results_['mean_test_R2']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_train_MSE']}\\t{search_test.cv_results_['mean_test_MSE']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_fit_time']*1e3}\\t{search_test.cv_results_['mean_score_time']*1e3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa1d87e-ccae-4a84-82a0-53a5991a68fc",
   "metadata": {},
   "source": [
    "Next, we optimize ESNs from PyESN using HAR as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fa2970df-2bbf-4514-a00b-e56c8de4ad2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'step1': {'esn__regressor__input_scaling': 0.24763754399239968, 'esn__regressor__spectral_radius': 1.4564326972237192}, 'step2': {'esn__regressor__n_reservoir': 50, 'esn__regressor__noise': 9.692658471430319}}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    search = load(f'./results/pyesn_har_seq_esn_h{H}.joblib')\n",
    "except FileNotFoundError:\n",
    "    initial_esn_params = {\n",
    "        'n_inputs': 1, 'n_outputs': 1, 'n_reservoir': 50, 'spectral_radius': 0.0, 'sparsity': 0.5,\n",
    "        'noise': 1e-3, 'input_shift': 0, 'input_scaling': 0.4, 'teacher_forcing': True,\n",
    "        'teacher_scaling': 1.,'teacher_shift': 0, 'random_state': 42}\n",
    "    esn_pipeline = Pipeline(steps=[(\"har_features\", har_features),\n",
    "                                   (\"scaler\", MinMaxScaler()),\n",
    "                                   (\"esn\", TransformedTargetRegressor(\n",
    "                                       regressor=PyESN(**initial_esn_params),\n",
    "                                       transformer=MinMaxScaler()))\n",
    "                                  ])\n",
    "    # Run model selection\n",
    "    step1_params = {'esn__regressor__input_scaling': uniform(loc=1e-2, scale=1),\n",
    "                    'esn__regressor__spectral_radius': uniform(loc=0, scale=2)}\n",
    "    step2_params = {'esn__regressor__n_reservoir': [50, 100, 200, 400, 800, 1600, 3200, 6400],\n",
    "                    'esn__regressor__noise': loguniform(1e-5, 1e1)}\n",
    "    \n",
    "    kwargs_step1 = {'n_iter': 200, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': scoring,\n",
    "                    \"refit\": \"R2\", \"cv\": ps, \"return_train_score\": True}\n",
    "    kwargs_step2 = {'n_iter': 200, 'random_state': 42, 'verbose': 1, 'n_jobs': -1, 'scoring': scoring,\n",
    "                    \"refit\": \"R2\", \"cv\": ps, \"return_train_score\": True}\n",
    "\n",
    "    searches = [\n",
    "        ('step1', RandomizedSearchCV, step1_params, kwargs_step1),\n",
    "        ('step2', RandomizedSearchCV, step2_params, kwargs_step2)]\n",
    "    search = SequentialSearchCV(esn_pipeline, searches=searches).fit(X, y)\n",
    "    dump(search, f'./results/pyesn_har_seq_esn_h{H}.joblib')\n",
    "print(search.all_best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18658d10-e66c-48f4-a722-a442364e3b37",
   "metadata": {},
   "source": [
    "We do an additional GridSearchCV without parameters, because the object returns a mass of useful information.\n",
    "\n",
    "If we do it without parameters, simply the estimator will be fit on the train test splits defined in our cv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b0c2346b-c3e5-477a-b5c5-8efe680171cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    search_test = load(f'./results/pyesn_har_seq_esn_h{H}_test.joblib')\n",
    "except FileNotFoundError:\n",
    "    search_test = GridSearchCV(estimator=clone(search.best_estimator_), param_grid={}, cv=ps_test,\n",
    "                               scoring=scoring, refit=\"R2\", return_train_score=True).fit(X, y)\n",
    "    dump(search_test, f'./results/pyesn_har_seq_esn_h{H}_test.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "12e9735a-32a1-43d9-be43-e5355df550f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 train\tR^2 test\tMSE train\tMSE test\tFit time in ms\tScore time in ms\n",
      "[0.55682322]\t[0.51334485]\t[-7.29200176e-08]\t[-7.87982402e-08]\t[104.21069463]\t[51.16319656]\n"
     ]
    }
   ],
   "source": [
    "print(f\"R^2 train\\tR^2 test\\tMSE train\\tMSE test\\tFit time in ms\\tScore time in ms\")\n",
    "print(f\"{search_test.cv_results_['mean_train_R2']}\\t{search_test.cv_results_['mean_test_R2']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_train_MSE']}\\t{search_test.cv_results_['mean_test_MSE']}\\t\"\n",
    "      f\"{search_test.cv_results_['mean_fit_time']*1e3}\\t{search_test.cv_results_['mean_score_time']*1e3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0587d289-0df2-4e3c-bd63-3d4a654b4e65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
